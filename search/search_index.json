{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Preamble","text":"<p>Greetings. You may know me as <code>11yu</code> from various botmaking / imagegen discords.</p> <p>This guide is mostly made with local generation in mind, and is not a good introduction to imagegen in general. You can check out some #other guides for that. Instead, I'll be focusing more on:</p> <ul> <li>1-2 steps further into the technical aspects of what a usual imagegen guide may provide.<ul> <li>This hopefully means somewhere between guides that say \"Use X (if Y)\" with little to no explanation, and those that dive head first into the meat and potatoes with heavy math equations and terminology hell.</li> </ul> </li> <li>lesser-known tips, tricks, and technologies that may improve your imagegen process.</li> </ul> <p>Everything here should be taken with a mountain of salt, as in practice, many other variables could influence your results greatly. Also, this article is mostly written by a dumb person (me). You will likely come out of a section thinking: \"Interesting. This doesn't help me at all.\"</p> <p>With that in mind, let's dive in.</p> <ul> <li>Samplers</li> <li>Schedulers</li> <li>Guidance</li> </ul>"},{"location":"Appendix/other_guides/","title":"Other Guides","text":"<ul> <li>Bex's Stable Diffusion Tips and Tricks; and General Usage Guide for Illustrious Models</li> <li>Skelly's Necronomicon: Art Generating</li> <li>StatuoTW's Guide to Making Bots - #Generating AI Art, a Guide to your First AI Gens.</li> </ul>"},{"location":"Appendix/references/","title":"References","text":"<p>Other materials referenced in the making of this rentry, that weren't already embedded into their respective sections. In no particular order:</p> <ul> <li>sander.ai</li> <li>huggingface's <code>diffusers</code></li> <li>A Comprehensive Review on Noise Control of Diffusion Model</li> <li>Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices</li> <li>Diffusion Models: A Comprehensive Survey of Methods and Applications</li> <li>Efficient Diffusion Models: A Survey</li> <li>Complete guide to samplers in Stable Diffusion</li> </ul>"},{"location":"Appendix/special_thanks/","title":"Special Thanks","text":"<p>My sincerest thank you, to all the people of this community. I would've never thought to make anything like this without yall.</p>"},{"location":"Guidance/00_guidance/","title":"What is Guidance?","text":"<p>UNDER CONSTRUCTION</p> <p>This section is incomplete and subject to massive changes. </p> <p>As originally conceived, there was no way to specify the sample generated by a diffusion model. For example, if your training images contained elves and bunnies, you can be sure that your model will generate either an elf or a bunny, but you can't specify which it should generate. If you wanted an elf specifically, your only hope was to run it on many different seeds and pray that one of those seeds made an elf.</p> <p>This is where \"Guidance\" comes in. Guidance is the umbrella term for any method that steers the output of the model during generation. This can be anything from text to other input images, OpenPose, etc.</p>"},{"location":"Guidance/01_cfg/","title":"CFG","text":"<p>CFG in Flux</p> <p>Flux was not trained with CFG, thus nothing in this section applies. The \"Guidance\" value you can provide to flux is not CFG.</p> <p>It's simplest to understand how CFG works exactly by digging through code:</p> <pre><code>cfg_result = uncond_pred + (cond_pred - uncond_pred) * cond_scale\n</code></pre> <p>Where <code>uncond_pred</code> is the model prediction without conditioning, and <code>cond_pred</code> is the model prediction with conditioning. <code>cond_scale</code> is CFG, and <code>cfg_result</code> is the result image we get. For brevity, I'll be referring to <code>uncond_pred</code> and <code>cond_pred</code> as <code>uncond</code> and <code>cond</code> respectively. You can imagine that:</p> <ul> <li><code>uncond</code> is what the model thinks it should do.</li> <li><code>cond</code> is what the model thinks it should do, given our guidance.</li> </ul> <p>Let's play with some CFG numbers:</p> cfg <code>cfg_result</code> effect 0.1 <code>uncond*0.9 + cond*0.1</code> The effect that our guidance has is pretty weak. 90% of the generation process is still decided by <code>uncond</code>. 0.5 <code>uncond*0.5 + cond*0.5</code> The strength of our guidance is on even footing with the unguided conditioning. 1 <code>cond</code> The <code>uncond</code> cancels out, leaving us with only <code>cond</code>. The generation process is entirely decided by our guidance. 2 <code>2*cond - uncond</code> The model actively moves away from <code>uncond</code>, while the effect that our guidance has increases even more. <p>When implementing CFG in practice, people also noticed what we found here - namely that when <code>cfg &gt; 1</code>, the model moves away from <code>uncond</code>. Then, couldn't we use <code>uncond</code> as some sort of opposite guidance - \"Do anything but this?\" Yes! This is what became negative prompts.</p> <p>Now with that in mind, let's rewrite the equation in more familiar terms:</p> <pre><code>denoised_image = negative + (positive - negative) * cfg\n</code></pre> <p>Where you can imagine <code>positive</code> and <code>negative</code> as the representation of the positive and negative prompts respectively that the model understands. Armed with this knowledge, let's reiterate what happens with prompts at various CFG levels:</p> <ul> <li><code>cfg &lt; 1</code>: Negative prompts would behave like positive prompts.</li> <li><code>cfg = 1</code>: Negative prompts have no effect. </li> <li><code>cfg &gt; 1</code>: The model will actively avoid generating anything in the negative prompt.</li> </ul> <p>I also want to note something special about <code>cfg = 1</code> - that is, the negative prompt having no effect. Couldn't we skip calculating <code>negative</code> entirely then? Yep. ComfyUI does this, which is why you'll see 2x iteration speed if you set <code>cfg = 1</code>.</p>"},{"location":"Guidance/01_cfg/#cfg_1","title":"CFG++","text":"<p>Rescales CFG to work in the range of 0 to 1 making it work better at low guidance scales. It also makes the diffusing process smoother, and may lead to reduced artifacts. One usually accesses these by choosing predefined samplers, i.e. in ComfyUI <code>euler_ancestral_cfg_pp</code> is <code>euler_ancestral</code> using CFG++.</p> <p>In my and Bex's personal tests:</p> <ul> <li>CFG++ samplers give eps-pred models a very good color range, rivaling that of what v-pred models claim to achieve. </li> <li>Using CFG++ to inpaint at high steps breaks the image for reasons unknown.</li> </ul> <p>Note that in ComfyUI, the <code>_cfg_pp</code> samplers in <code>KSampler</code> are alt implementations where you instead simply want to divide the CFG you're used to by 2. For example, if you usually run <code>euler_ancestral</code> at CFG=7, you'd run <code>euler_ancestral_cfg_pp</code> at CFG=3.5. In practice, the reasonable range I find for these is CFG between <code>[0.7, 3]</code>, and <code>1.6</code> is a sane default to start with.</p>"},{"location":"Models/00_models/","title":"What is a Model?","text":"<p>A generative model learns to create new data similar to the training data. This is in contrast to a discriminatory model which learns to look at a new data point and predict a label or number. For images, you can imagine that:</p> <ul> <li>generative model: learns to make new images</li> <li>discriminatory model: learns to classify it as cat or dog / predict auction price</li> </ul> <p>In modern day, generative models are usually deep neural networks, including GANs, VAEs, and of course diffusion models.</p>"},{"location":"Models/00_models/#what-does-generating-mean","title":"What Does \"Generating\" Mean?","text":"<p>Researchers mathematically formalize the idea of generating data as sampling from a probability distribution \\(p_\\text{data}\\). A probability distribution is just something that assigns each possible outcome of an event with the chance of it happening. More specifically, \\(p_\\text{data}\\) is a function that takes in a data point and gives you how likely it is to occur.</p> <p>Illustrative Example</p> <p>Imagine rolling a fair die. The possible outcomes are one of the 6 faces, with a number 1-6 on that side. The probability of each outcome is equal. The probability distribution of the dice roll \\(p_\\text{dice}\\) could thus be described as the following table:</p> \\(\\text{roll}\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) \\(p_\\text{dice}(\\text{roll})\\) \\(\\frac16\\) \\(\\frac16\\) \\(\\frac16\\) \\(\\frac16\\) \\(\\frac16\\) \\(\\frac16\\) <p>Let's say we try to generate cats. One can imagine there that there exists an underlying probability distribution, \\(p_\\text{CatImages},\\) which assigns a higher probability to natural cat images, and low probability for other images like dogs. If we can sample from \\(p_\\text{CatImages},\\) that's the same as being able to generate new, high quality cat images.</p> <p>The goal of generative modeling can now be formalized as to allow us to sample from \\(p_\\text{data},\\) when \\(p_\\text{data}\\) itself could be extremely complicated.</p>"},{"location":"Models/00_models/#p_textdata-for-images","title":"\\(p_\\text{data}\\) for Images","text":"<p>Imagine 2x2 images with 1 grayscale channel, where the channel value goes from 0-255. Each image can be thought of as a \\(2\\times2\\times1\\) list of numbers. For example, this could be a black and white image where the top left pixel has brightness 100, bottom right 200, and the rest completely dark:</p> <p>\\(\\begin{bmatrix}[100] &amp; [0] \\\\ [0] &amp; [200]\\end{bmatrix}\\)</p> <p>We can also assign each image with a probability, thus creating a probability distribution for images:</p> \\(\\text{Image}\\) \\(\\begin{bmatrix}[100] &amp; [0] \\\\ [0] &amp; [200]\\end{bmatrix}\\) \\(\\begin{bmatrix}[42] &amp; [0] \\\\ [0] &amp; [42]\\end{bmatrix}\\) ... \\(p_\\text{data}(\\text{Image})\\) \\(\\frac1{1234}\\) \\(\\frac{11}{1337}\\) ... <p>Real world images are obviously more complex, having way bigger widths and heights, and having multiple color channels like RGB, resulting in \\(W\\times H\\times 3\\) list of numbers. However, the principles stay fundamentally the same. </p>"},{"location":"Models/00_models/#why-modeling-p_textdata-is-a-problem","title":"Why Modeling \\(p_\\text{data}\\) is a Problem","text":"<p>Great! So, just train a neural network to learn \\(p_\\text{data},\\) right? Well... it's not so easy. To see why, let's try designing a neural network that represents a probability distribution. </p> <p>Let's call our neural network \\(\\text{NN}(\\text{Input}),\\) and let's call the distribution that \\(\\text{Input}\\) comes from \\(p_\\text{init}.\\) \\(p_\\text{init}\\) can pretty much be anything; You could for example train a neural network to turn cat images into dog images if you really wanted to.</p> <p>In practice, \\(p_\\text{init}\\) is usually the gaussian distribution, aka the noise distribution. The benefit is that the gaussian is very simple, and we know how generate infinite samples from it. So if our neural network learns to turn \\(p_\\text{init}\\to p_\\text{data},\\) and we know how to generate infinite samples of \\(p_\\text{init},\\) we can now generate infinite samples of \\(p_\\text{data}.\\) Neat!</p> <p>Assumptions</p> <p>Since it's almost always the case that \\(p_\\text{init}\\) is the gaussian, I'll be making this assumption in this guide from here on.</p> <p>Since our outputs should be probabilities, these outputs should all be positive, as negative probability doesn't really make sense, right? A conventional way to do this is to transform the output using the exponential function, i.e. turn it into \\(e^{-\\text{NN}(\\text{Input})}.\\) The details are mathy, but the important part is that \\(e^{-\\text{NN}(\\text{Input})}\\) is always positive.</p> <p>Next, recall that \\(p_\\text{data}\\) gives you the chance of a specific outcome of an event. Thus, we should expect that the sum of all these chances - that there being an outcome, any outcome - be 1. To do this, we can divide our outputs by a magical constant \\(Z\\). Our output is now \\(\\frac{e^{-\\text{NN}(\\text{Input})}}Z.\\) \\(Z\\) is called the normalizing constant in literature.</p> <p>To recap:</p> <ul> <li>Neural network outputs \\(\\text{NN}(\\text{Input})\\) which should be similar to \\(p_\\text{data},\\) where \\(\\text{Input}\\) comes from some initial distribution \\(p_\\text{init},\\) usually the gaussian (the noise distribution).</li> <li>Probabilities should be positive, so we instead use \\(e^{-\\text{NN}(\\text{Input})}\\) as the output</li> <li>The sum of probabilities should be 1, so we instead use \\(\\frac{e^{-\\text{NN}(\\text{Input})}}Z\\) as the output</li> </ul> <p>The problem lies in \\(Z,\\) which is next to impossible to calculate for a general neural network. People have come up with ways around this of course. Approaches prior to diffusion can be split into 3 categories, referencing this talk / blog by Yang Song, one of the pioneers of modern diffusion:</p> <ol> <li>Approximate \\(Z\\): Calculating \\(Z\\) exactly is hard, so just use an approximation. One problem is this approximation is still expensive to calculate.</li> <li>Restrict the Neural Network: Restrict the neural network to specific architectures so \\(Z\\) can be calculated. Obviously this limits our freedom in designing the neural network. Examples include autoregressive models.</li> <li>Model the Generation Process Only: Skip trying to model \\(p_\\text{data}\\) entirely, just model a process that can generate new data. This usually involves using adversarial loss, which is highly unstable and hard to train with, plus the result might deviate from \\(p_\\text{data}\\). Examples include GANs.</li> </ol>"},{"location":"Models/00_models/#what-is-a-loss-function","title":"What is a Loss Function?","text":"<p>At a high level, we always say we \"train a model to learn something.\" Well... how do we quantify how much the model has \"learned\"?</p> <p>The loss (function) \\(L\\), also called the cost or objective, is a function people design to measure how well the model performs on a task. Conventionally, a lower loss means the model is performing better. Training a model could also be referred to as minimizing the loss.</p> <p>Illustrative Example</p> <p>Let's say you're trying to predict house prices. A simple and common loss function for this task could be \\(L=|\\text{TruePrice} - \\text{PredictedPrice}|^2\\), the square of the difference between the true price and the predicted price. As you can imagine, trying to minimize the loss \\(L\\) is the same as trying to reduce the difference between the true price and the prediction, or in other words make the prediction more accurate. </p> <p>Why Minimize the Square?</p> <p>You may ask, why minimize the difference squared and not just the difference? An intuitive explanation is this: If the difference between the true price and the predicted price is big, then the square will extrapolate it to be bigger. This means we punish the model way harder if it makes a wildly inaccurate prediction.</p> <p>There are more math-heavy reasons rooted in statistics, the details of which are out of the scope of this article. (For those interested in searchable keywords, minimizing the squared difference - the L2 loss - corresponds to maximizing the likelihood, under the assumption that the random errors are normally distributed.)</p>"},{"location":"Models/00_models/#what-is-adversarial-loss","title":"What is Adversarial Loss?","text":"<p>Adversarial loss generally refers to when practitioners pit two neural networks against each other.</p> <p>For example, in image genereating GANs, two models are simutaneously trained at once - the generator \\(G\\), and the discriminator / adversary \\(A\\):</p> <ul> <li>\\(G\\) tries its best to create realistic images and fool \\(A\\).</li> <li> <p>\\(A\\) tries its best to distinguish between real and generated images.  This is a sound approach, and GANs have been SOTA in terms of generative modeling. It comes with its own problem though, most prominently that it's very hard to balance \\(G\\) and \\(A\\). For example:</p> </li> <li> <p>\\(G\\) only learns how to exploit \\(A\\)'s defects, creating \"images\" that trick \\(A\\) but are completely unnatural.</p> </li> <li>\\(G\\) only learns a few types of images that \\(A\\) is less certain about, destroying output variety.</li> <li>\\(A\\) is too good at discerning real vs. generated that makes it impossible for \\(G\\) to learn from gradient descent.</li> <li>\\(G\\) and \\(A\\) end up in an infinite XYZ cycle. \\(G\\) learns to generate only X, so \\(A\\) learns to classify that as generated; \\(G\\) then learns to only generate Y, so \\(A\\) classifies all Y as generated, repeat.</li> </ul> <p>Several training rules, different types of losses, regularization techniques... have been proposed just to attempt to solve this problem in GANs.</p>"},{"location":"Models/01_diffusion/","title":"A Premier on Diffusion","text":"<p>Sources</p> <p>This entire guide mostly references MIT's lecture notes, and thus might differ from how other literature formuate things.</p> <p>For example, since flow could be thought of as a special case for diffusion, I'll be referring to both as \"diffusion\" here.</p> <p>As mentioned in #what is a model, generative models can be formalized as models that allow us to sample from the data's underlying probability distribution \\(p_\\text{data}.\\) Diffusion is the latest development.</p>"},{"location":"Models/01_diffusion/#motivation-for-diffusion","title":"Motivation for Diffusion","text":"<p>Recall from #the issue in modeling \\(p_\\text{data}\\) that we trained a neural network \\(\\text{NN}(\\text{Input}),\\) and use \\(\\frac{e^{-\\text{NN}(\\text{Input})}}Z\\) as our output to (1) force our outputs be positive since probabilities should be positive, and (2) force that the sum of all possible outputs be 1 since it's guaranteed for there being an outcome to any event.</p> <p>The issue lies in \\(Z\\) being hard to calculate. Remedies have been designed, yet all come with undesirable downsides. Diffusion solves these issues, specifically:</p> <ol> <li>Diffusion doesn't have to calculate nor approximate \\(Z.\\)</li> <li>Diffusion imposes no restrictions on neural network architecture, nor does it use the unstable adversarial loss.</li> <li>Diffusion still models \\(p_\\text{data},\\) ensuring samples are actually similar to the training examples. </li> </ol> <p>These strong foundations lead to them becoming SOTA in fields like image and video generation, and enabling tasks like inpainting to be done.</p>"},{"location":"Models/01_diffusion/#how-2-diffuse","title":"How 2 Diffuse","text":"<p>Diffusion is the process of adding noise to an image, gradually destroying information until it becomes pure noise. To be more concrete, I'll define the timestep \\(t\\):</p> <ul> <li>\\(t=0\\) is the beginning of the diffusion process, where no noise has been added and the image \\(x_0\\) is perfectly clean.</li> <li>\\(t=1\\) is the end of the diffusion process, where the image \\(x_1\\) is just pure noise (all of the information from \\(x_0\\) has been destroyed).</li> <li>Any other \\(t\\) is during the process, where \\(x_t\\) is some combination of data and noise.</li> </ul> <p>A diffusion model learns to reverse the diffusion process.  Specifically, diffusion models are trained to learn \"paths,\" which begin at a pure noise input \\(x_1\\) and end at a data point \\(x_0\\)  that could realistically come from \\(p_\\text{data}\\) (for example, a cat image taken from the distribution of cat images). Following these paths is the same as gradually transforming noise into clean images.</p> <p>These paths can be naturally described by something called a differential equation, which is a fancy name for an equation that relates something with how it should change. For example, let's say you're half way through generating a cat image with diffusion, so you have a half-noise-half-cat image \\(x_{0.5}.\\) The differential equation would tell you how you should change \\(x_{0.5}\\) to get closer to a real cat image: these pixels should be a bit brighter, those should be less saturated, ... etc.</p> <p>Analogy</p> <p>Imagine flying to France, and you just landed at the airport (\\(x_1\\)) and you want to get to a restaruant (\\(x_0\\)), but you don't have a map. What you can do is ask people on the street (the differential equation) for directions, walk in that direction for a bit, then ask people where to go next.</p> <p>There is a tradeoff: Asking people more often costs more time, but the directions will be more accurate. Asking people less often and walking a big distance at once will save you a lot of time interacting with people, but you may overshoot, turn the wrong corner, etc. </p> <p>The same is with solving an ODE / SDE: Higher step size = fewer differential equation evaluations = less time spent and less accurate. Vice versa.</p> <p>Differential equations come in 2 kinds: stochastic (SDE) or ordinary (ODE). The basic difference is that SDEs include a noise term while ODEs don't. This also leads to the concept of SDEs being non-convergent, where one can observe that a stochastic sampler like <code>euler_ancestral</code> never really \"settle\" on an image and increasing step counts can drastically change the final image composition.</p> <p>To generate an image then is to solve one of these SDEs or ODEs. This also means that all samplers can be thought of as SDE or ODE solvers, and SDE or ODE solvers developed before diffusion, like the Runge-Kutta methods, can be used as samplers.</p> <p>On ODE / SDE Sampling of Diffusion / Flow</p> <p>While denoising diffusion and flow matching were formulated using SDEs and ODEs respectively, it has been shown that one can construct an equivalent ODE / SDE from the other, like in Song et. al. for example.</p> <p>This means that when implemented correctly, you can use ODE methods with denoising diffusion and SDE methods with flow. </p>"},{"location":"Models/01_diffusion/#latents-why-and-how","title":"Latents - Why and How","text":"<p>Directly working with images is costly - each image has thousands of pixels, each with their rgb values.</p> <p>Latents are compressed representation of images that still retain most of the information, like colors, composition, etc. Intuitively, think of converting images to their latents as jpg compression - you lose a little detail in exchange for massively reduced file size.</p> <p>Thus, latent diffusion was born - it's basically just diffusion + using latents. This significantly reduces the hardware reqs for both training and inferencing, which is why they dominate the market. <code>SD</code>, <code>Flux</code>, you name it, it probably uses latents.</p> <p>How do we compress and decompress an image from / to its latent? Well, we don't know... but we can train a neural network to do it for us! For this, Variational AutoEncoders (VAEs) are the dominant players. </p> <p>VAE En/Decode is Lossy</p> <p>Like jpg compression, turning images into latents and back is \"lossy\" - that is, you lose a bit of information in the process. </p> <p>This often comes up when doing many inpainting iterations: you do not want to en/decode the same image many times, lest the image quality slowly but surely degrades.</p>"},{"location":"Models/01_diffusion/#practical-details","title":"Practical Details","text":"<p>Modern VAEs usually compress 8x8 = 64 normal pixels into 1 latent pixel. This is also why you can only specify image widths and heights in multiples of 8 - the VAE decode can only produce images whose sizes are multiples of 8.</p> <p>Each normal pixel usually has 3 channels: red, green, and blue. Each latent pixel has differing amounts of channels depending on model. Having more channels per latent pixel means more information could be retained, but the hardware reqs are increased.</p> <p>Originally, most decided to go with a 4-channel VAE, including <code>SD 1.X</code> and <code>SDXL</code>. In recent times, there has been a move towards higher channel VAEs. <code>Flux</code>, <code>SD 3.X</code>, <code>Lumina 2.0</code>, all use 16 channel VAEs. Even more recently, some have ditched latent space and gone back to directly generating in pixels, such as <code>PixelFlow</code>.</p>"},{"location":"Models/01_diffusion/#references","title":"References","text":"<ul> <li>Loss Functions in Diffusion Models: A Comparative Study</li> <li>(Karras) Elucidating the Design Space of Diffusion-Based Generative Models</li> <li>(Song) Score-Based Generative Modeling through Stochastic Differential Equations</li> </ul>"},{"location":"Models/02_training_objectives/","title":"Training Diffusion Models","text":"<p>Recap: #How Diffusion Generates Images</p> <p>A diffusion model learns \"paths\" that begin at some initial point \\(x_1\\) which is pure noise, and ends at a data point \\(x_0\\) from \\(p_\\text{data}\\) (for example a cat image from \\(p_\\text{CatImages}\\)). By \"following\" these learned paths, gaussian noise can be transformed into cat images.</p> <p>Mathematically, these paths are described by differential equations (DE), coming in two types: ordinary (ODE) and stochastic (SDE). To follow a path is to solve the corresponding DE.</p>"},{"location":"Models/02_training_objectives/#flow-and-score-matching-loss","title":"Flow and Score Matching Loss","text":"<p>A diffusion model usually is designed to have two inputs, a noisy image \\(x\\) and the timestep \\(t.\\) To train the diffusion model, there are two common options, flow matching or score matching:</p> Aspect Flow Matching Score Matching Type ODE SDE Loss \\(\\|u_\\text{predicted}(x,t)-u_\\text{true}(x,t)\\|^2\\) \\(\\|s_\\text{predicted}(x,t)-s_\\text{true}(x,t)\\|^2\\) Predicted Quantity Think of \\(u\\) as the \"velocity.\" Given your position (\\(x\\)) and how far along the path you are (\\(t\\)), \\(u\\) is how fast and in which direction to walk to eventually get to the path's end. \\(s\\) is the score, defined as the gradient of the log of the probability distribution \\(s(x,t)=\\nabla_x\\log p_t(x,t).\\) Note that technically SDE needs both \\(u\\) and \\(s\\) to work. Under Practical Conditions...* \\(u_\\text{true}(x,t)\\) is simply defined as \\(x_0-x_1,\\) which is very stable to train on. (Also relating to something called the Optimal Transport in literature) You don't actually need to train two models. One option is to make the neural network output two things (\\(u\\) and \\(s\\)) at the same time. Also see below. Models Most modern models use this, like <code>SD 3.X</code>, <code>Flux</code>, <code>Lumina</code>, <code>Qwen-Image</code>, <code>Wan</code>, etc. The original SD releases are noise predictors, like <code>SD 1.X</code>, <code>SDXL</code>, etc. <p>*Stil under the same practical conditions: </p> <ul> <li>\\(u\\) and \\(s\\) can be converted between each other. <ul> <li>This means you can e.g. use SDE sampling with a flow matching model by doing the correct math conversion; A sampler designed for one can be used on the other, etc.</li> <li>Another way to bypass needing two models for score matching, is only train a \\(u\\) predictor then turn it into \\(s\\) with math afterwards, or vice versa.</li> </ul> </li> <li>A score matching model is equivalent to a noise prediction model which predicts the noise to remove from images. Thus, these models are also called denoising diffusion models.</li> </ul> <p>Since the \"practical conditions\" are almost always met in practice, and basically all models on the market satisfy them, I'll be assuming they're true from here on. </p> <p>What are these practical conditions?</p> <p>I've skipped the details in the main text since it's math heavy, and you can assume that they're satisfied most of the time anyway, but the conditions are that:</p> <p>The models have Gaussian probability paths, which mathematically have the form of \\(\\mathcal N(\\alpha_t z; \\beta_t^2I_d),\\) where \\(\\alpha, \\beta\\) are noise schedulers (monotonic, continuously differentiable, and \\(\\alpha_1=\\beta_0=0\\) and \\(\\alpha_0=\\beta_1=1\\)).</p>"},{"location":"Models/02_training_objectives/#hurdles-in-training-with-score-matching","title":"Hurdles in Training with Score Matching","text":""},{"location":"Models/02_training_objectives/#score-matching-to-noise-prediction","title":"Score Matching to Noise Prediction","text":"<p>When an image is close to being clean, score matching loss becomes numerically unstable and training breaks. Remember that I'm assuming practical conditions, then the score matching loss becomes the following (simplified):</p> \\[ L=\\underset{\\substack{\\downarrow \\\\ \\text{Near 0 when} \\\\ \\text{image is} \\\\ \\text{almost clean}}}{\\color{red}\\frac1{\\beta_t^2}}|\\beta_ts_\\text{predicted}(x,t)+\\epsilon|\\Rightarrow\\text{divide by 0 error} \\] <p>Thus, a \"score matching\" model is very often reparameterized (basically, changed) and trained on a different but still mathematically equivalent objective. </p> <p>DDPM drops the red part of the original loss, and reparameterizes the score matching model into a noise prediction model (\\(\\epsilon\\)-pred, eps-pred). eps-pred saw widespread adoption afterwards.</p> \\[L=|\\epsilon_\\text{predicted}(x,t)-\\epsilon|\\]"},{"location":"Models/02_training_objectives/#noise-prediction-to-tangential-velocity-prediction","title":"Noise Prediction to (Tangential) Velocity Prediction","text":"<p>eps-pred becomes a problem again in few-steps sampling. At the extreme of 1 step, we're trying to generate a clean image from pure noise, however the eps-pred model only predicts the noise to remove from an image. Removing noise from pure noise results in... nothing. Empty. Oops, that's bad.</p> <p>That's the problem the authors of this work faced. They propose a few reparameterizations that fix this, the most influential of which being (tangential) velocity prediction (v-pred): </p> \\[L=|v_\\text{predicted}(x,t)-v|,\\quad v=\\alpha_t\\epsilon - \\beta_t x_0\\] <p>Tangential Velocity \\(v\\) and Velocity \\(u\\)</p> <p>You might remember that there was also a \"velocity\" \\(u,\\) that being what the flow matching models predict. On the other hand, v-pred is also often also called velocity prediction. How do they relate to each other?</p> <p>Confusingly, they really don't. v-pred comes from an angular parameterization, where you can find a visual in the same paper here. </p> <p>Essentially, this loss is saying that at high noise levels, the v-pred model should focus on trying to make an image, and at low noise levels it should instead focus on removing the remaining noise.</p>"},{"location":"Models/02_training_objectives/#current-schedules-are-bad","title":"\"Current Schedules are Bad\"","text":"<p>It was found that for most noise schedules at the time, \\(x_1\\) which is supposed to be pure gaussian noise still left a bit of the data in it, resulting the model learning very low frequency information when it shouldn't have. </p> <p>It's likely that models suffering from flawed training schedules learned the average value of the channels correlates with the average brightness of the clean image. Then since when sampling we start with pure gaussian noise (whose channel averages are 0), models always generated images with around the same brightness.</p> <p>Fixing this was pretty simple, just make \\(x_1\\) actually pure noise when training. They also recommend switching to v-pred, since again eps-pred can't learn from pure noise images.</p>"},{"location":"Models/04_vae/","title":"VAE (Variational AutoEncoder)","text":"<p>UNDER CONSTRUCTION</p>"},{"location":"Models/99_notes/","title":"Additional Notes","text":"<p>UNDER CONSTRUCTION</p>"},{"location":"Models/99_notes/#in-relation-with-literacy","title":"In Relation with Literacy","text":""},{"location":"Sampling/00_sampling/","title":"What is Sampling?","text":"<p>Under Construction</p> <p>As said in #what is generating, sampling means to pick data out according to a data distribution; A data distribution is something that assigns probabilities to data, for example a cat image distribution \\(p_\\text{CatImages}\\) would assign high probability to cat images, and low probability to everything else like dog images.</p> <p>As said in #how 2 diffuse, for diffusion models, sampling is done through solving the corresponding ODE/SDE that would transform an input from an initial distribution, like noise from a gaussian, into a clean image, like a cat image from a cat distribution.</p>"},{"location":"Sampling/00_sampling/#what-is-a-sampler","title":"What is a Sampler?","text":"<p>While not all of them were conceived this way, all samplers can be thought of as something that can solve the diffusion ODE/SDE.</p>"},{"location":"Sampling/01_accuracy/","title":"\"Accuracy / Control\"","text":"<p>You've probably seen something like this in other diffusion guides:</p> <p><code>DPM++</code> is better [than <code>Euler</code>] for \"accuracy\" / \"precision\" / \"control\".</p> <p>Viewing sampling through the lens of solving the diffusion differential equation (DE), it becomes clearer what this could mean - to solve the DE more accurately. In math terms, we'd say that the more accurate solver is higher order.</p> <p>However, note we're simply looking to generate nice images. Numerical accuracy does not directly translate to good-looking images.</p> <p>For example, the ever-popular <code>euler(_ancestral)</code> is actually the most inaccurate sampler there is. The errors it make manifest as blurring the output and create a soft/dreamy visual style that many find pleasing. </p> <p>Inaccuracies don't always play out nicely though. For example, some potential drawbacks may include:</p> <ul> <li>Small detail is lost: backgrounds merging into meaningless blobs, hair strands losing definition, etc.</li> <li>Worse prompt adherence: In severe cases, the errors could become so big that it actively hurts how much the image follows your prompt.</li> </ul> <p>On the other hand, diffusion DEs are very stiff, especially at high CFG - this increases numerical instability and in practice:</p> <ol> <li>Makes <code>adaptive</code> solvers take tiny steps = very long to generate an image</li> <li>Makes higher order solvers unstable and do worse, completely breaking in severe cases, especially at low steps</li> </ol> <p>This may be why the community favorites - <code>euler_ancestral</code> and <code>dpmpp_2m</code> - are \"only\" first-order and second-order respectively. (And <code>dpm(pp)</code> uses a clever math trick called the exponential integrator to make it less stiff)</p> <p>Info</p> <p>Did you know that as originally formulated in DDPM, a diffusion model has to take 1000 steps to generate an image? What we're doing now all seems like \"low step count\" compared to that!</p>"},{"location":"Sampling/01_accuracy/#how-does-order-affect-error","title":"How Does Order Affect Error?","text":"<p>Oversimplification</p> <p>I'll be brushing over many details and oversimplifying things for ease of understanding here. For more accurate information on this topic, see truncation error.</p> <p>Order measures how much the errors a sampler makes scale down when you decrease the step size - or equivalently, increase the number of steps.</p> <p>Let's assume for simplicity that the error of any sampler taking 1 step is 10. As in, by some measure, the difference between the truth and the answer produced by a sampler is 10.</p> <p>Now, let's take <code>euler</code>, <code>heun</code>, and <code>bosh3</code>, which have an order of 1, 2, 3 respectively, and look at the error at various steps:</p> steps <code>euler</code> <code>heun</code> <code>bosh3</code> 2 <code>10 / 2</code> = 5 <code>10 / (2*2)</code> = 2.5 <code>10 / (2*2*2)</code> = 1.25 3 <code>10 / 3</code> \u2248 3 <code>10 / (3*3)</code> \u2248 1.1 <code>10 / (3*3*3)</code> \u2248 0.37 4 <code>10 / 4</code> = 2.5 <code>10 / (4*4)</code> = 0.625 <code>10 / (4*4*4)</code> \u2248 0.156 <code>n</code> <code>10 / n</code> <code>10 / (n*n)</code> <code>10 / (n*n*n)</code> <p>In general, if the order of a sampler is <code>O</code>, and the error it makes when you take 1 step is <code>E</code>, then the error if you take <code>N</code> steps would be around <code>E / (N^O)</code>.</p> <p>Let's compare <code>euler</code> and <code>heun</code>. <code>heun</code> takes twice as long as <code>euler</code> per step, and has an order of 2. Now, let's run them for the same amount of time and see what happens to the error:</p> <ul> <li><code>euler</code> for 10 steps, the error is about <code>10 / 10</code> = 1</li> <li><code>heun</code> for 5 steps (because it takes 2x as long), the error is about <code>10 / (5*5)</code> = 0.4</li> </ul> <p>So in theory, you can get better bang (accuracy) for your buck (time) by using higher order samplers. </p> <p>The high stiffness of diffusion DEs makes out-of-the-box high-order samplers do poorly though. Using stiff-resistant techniques is recommended, for example:</p> <ul> <li>Implicit methods like <code>gauss-legendre</code></li> <li>Exponential integrators like <code>deis</code> (and most ODE samplers that came after, including the <code>dpm(pp)</code> family, <code>res</code> (Refined Exponential Solver), etc.)</li> </ul>"},{"location":"Sampling/03_sample_speed/","title":"Generation Speed and How to Go Faster","text":"<p>Running the neural network is the most computationally expensive operation by far, when it comes to generating images with diffusion. The time taken for a generation is mostly determined by how many times the neural network needs to be ran. Running the neural network is also called \"calling/evaluating the model,\" doing a \"model call/evaluation,\" etc.</p> <p>This is why in a sampler speed comparison chart, one can notice that most of they seem to run in integer multiples of <code>euler</code> speed, e.g. <code>heun</code> is about 2x slower than <code>euler</code>: <code>euler</code> runs the model once per step, making it a great unit to compare against; <code>heun</code> runs the model twice per step, hence it's 2x slower.</p> <p>This is also why researchers don't compare samplers in number of steps, but Number of Function Evaluations (NFE), or in other words how many times the model was run. It wouldn't make practical sense to compare say <code>euler</code> and <code>heun</code> both at 20 steps, because the latter would have twice the NFE and run for twice as long.</p> <p>Steps is Not Time Spent</p> <p>Be on the lookout for potentially misleading statements, such as: \"<code>dpmpp_sde</code> is great for low steps sampling.\" While technically true, <code>dpmpp_sde</code> runs the model twice per step, which means that in the time <code>dpmpp_sde</code> runs for 5 steps, you could've run say <code>dpmpp_2m</code> for 10 steps. Make sure to take this into account when making speed-quality comparisons.</p>"},{"location":"Sampling/03_sample_speed/#some-things-affecting-sampling-speed","title":"Some Things Affecting Sampling Speed","text":"<p>Certain samplers run the model multiple times per step to achieve higher accuracy. See the #sampler tl;dr for details.</p> <p>Increasing the image resolution means the model has to ingest and output more information, making it take longer.</p> <p>In each step of generating an image, the model has to predict how to change the current intermediate output to move it closer to an image described by the positive prompt.</p> <ul> <li>Enabling negative prompts by setting #cfg to anything but 1 doubles the amount of work and thus halves the speed. The model now additionally needs to predict on the negative prompt (to move away from it).</li> <li>Perp-Neg makes negative prompts very strong, but the model now also needs to predict on the empty prompt. This means 1.5x slower than cfg-not-1 and negative prompts are in play, or 3x slower than if cfg is 1.</li> </ul>"},{"location":"Sampling/03_sample_speed/#some-things-not-affecting-sampling-speed","title":"Some Things NOT Affecting Sampling Speed","text":"<p>Schedulers only determine the noise levels of each step, the number of model runs should be unaffected.</p> <p><code>ancestral</code> or <code>sde</code> variants need to spend a bit of time manipulating the noise each step, but the time used for this is tiny compared to calling the model. They largely run as fast as their original version.</p> <p><code>cfg_pp</code> variants redo how cfg works, but doesn't change the number of model calls.</p>"},{"location":"Sampling/03_sample_speed/#table-of-sampler-speeds","title":"Table of Sampler Speeds","text":"<p>For convenience, below is a table of commonly found samplers and how much longer they take to complete 1 iteration, relative to <code>euler</code>. 1x means about the same speed, 2x means twice as slow, etc. </p> How much slower Sampler 1x <code>euler, euler_cfg_pp, euler_ancestral, euler_ancestral_cfg_pp, dpm_fast, dpmpp_2m, dpmpp_2m_cfg_pp, dpmpp_2m_sde, dpmpp_2m_sde_gpu, dpmpp_3m_sde, dpmpp_3m_sde_gpu, ddpm, lcm, ipndm, ipndm_v, deis, res_multistep, res_multistep_cfg_pp, res_multistep_ancestral, res_multistep_ancestral_cfg_pp, gradient_estimation, gradient_estimation_cfg_pp, er_sde, sa_solver, ddim, uni_pc, uni_pc_bh2</code> 2x <code>heun, dpm_2, dpm_2_ancestral, dpmpp_2s_ancestral, dpmpp_2s_ancestral_cfg_pp, dpmpp_sde, dpmpp_sde_gpu, seeds_2, sa_solver_pece</code> 3x <code>heunpp, seeds_3</code>"},{"location":"Sampling/04_types/","title":"Types of Sampling Methods","text":""},{"location":"Sampling/04_types/#adaptive","title":"<code>adaptive</code>","text":"<p>Samplers that choose their own steps, ignoring your setting for step count and scheduler. In some implementations, <code>steps</code> may instead be used as the \"max steps\" before it's forcefully stopped lest it takes too long.</p>"},{"location":"Sampling/04_types/#stochastic-sde-ancestral-a-restart","title":"Stochastic (<code>SDE</code>, <code>ancestral (a)</code>, <code>Restart</code>)","text":"<p>Samplers that inject noise back into the image. They never converge - with higher and higher step counts, they don't land on 1 final image and keep refining, instead, the composition may drastically change even if it's very late down the line.</p> <p>The theoretical quality of images generated based on non-stochastic vs. stochastic sampling depends on step count:</p> <ul> <li>low steps: samplers make a few big errors (low steps, high step size). Non-stochastic samplers usually make errors smaller than stochastic samplers if you compare 1 step of each. Thus, non-stochastic methods do better than stochastic methods in low steps.</li> <li>high steps: samplers make many small errors (high steps, small step size), which build up over time. It's now the accumulated error affecting image quality the most, and the random noise introduced by stochastic methods can gradually correct them. Thus, stochastic methods do better than non-stochastic methods in high step counts.</li> </ul> <p>Stochastic Methods In Low Steps</p> <p>In practice, it's almost always better to use stochastic samplers if you don't care about non-convergence.</p> <p>Many new stochastic methods also try to incorporate the best of both worlds, working nicely even in low steps. This includes <code>Restart</code>, <code>er_sde</code>, <code>sa_solver</code>, and <code>seeds</code>.</p>"},{"location":"Sampling/04_types/#why-stochasticity-break-flux-and-more","title":"Why Stochasticity Break Flux and More","text":"<p>SD 3.X, Flux, AuraFlow, Lumina 2, and potentially more to come, all use an architecture based on Rectified Flow (RF), which is very sensitive to the variance (a statistical measure) of the data. </p> <p>Without careful calibration, chances are that your stochastic sampler makes the variance increase without bound, thus breaking these models. This is why people weren't having luck using anything <code>ancestral</code> or <code>sde</code> etc. on them.</p>"},{"location":"Sampling/04_types/#singlestep-s-multistep-m","title":"<code>singlestep (s)</code> / <code>multistep (m)</code>","text":"Feature Singlestep (s) Multistep (m) How it works Runs the model multiple times each step for better accuracy Considers multiple previous steps for better accuracy Model Calls per Step <code>k</code> 1 Speed (per step) <code>k</code> times slower than 1 <code>euler</code> Basically as fast as <code>euler</code> Accuracy High Lower than <code>singlestep</code> Example <code>dpmpp_2s_ancestral</code> (2 model calls per step = 2x slower than <code>euler</code>) <code>dpmpp_2m</code> (same speed as <code>euler</code>)"},{"location":"Sampling/04_types/#implicit-explicit","title":"Implicit / Explicit","text":"<p>2 approaches used to solve DEs. </p> <p>Implicit methods solve a harder form of the DE, making them slower but more resistant to stiffness. This means in theory, you can use higher order implicit methods without them breaking, leading to moar accuracy. (This is super slow, though.)</p> <p>ALL common samplers are explicit. This includes <code>euler</code>, <code>deis</code>, <code>ipndm(_v)</code>, <code>dpm(pp)</code> family, <code>uni_pc</code>, <code>res_multistep</code>, and more.</p> <p>The quality-speed tradeoff of implicit methods seems to limit their popularity. They're also not found as defaults in popular UIs.</p> <p>Where Can I Find Implicit Samplers?</p> <p>ComfyUI: RES4LYF</p>"},{"location":"Sampling/04_types/#diagonally-implicit","title":"Diagonally Implicit","text":"<p>These are a subset of implicit methods whose difficulty to solve sits between implicit and explicit methods. They're easier to solve, but can't get as accurate.</p> <p>Here's a comprehensive review of Diagonally Implicit Runge Kutta (DIRK) methods for the interested.</p>"},{"location":"Sampling/04_types/#training-free-training-based","title":"Training-Free / Training-Based","text":"<p>Training-free methods are those that you can use without further changing the model. You can simply load the model in and use a training-free sampling method on it and it'll (probably) work. These include familiar faces like <code>euler</code>, <code>dpmpp_2m</code>, etc.</p> <p>Training-based methods require further modification of the model. This would include LCM, Lightning, Hyper, etc. where in order to use them you need to download a separate version of a model or use a LoRA. The upside is generating images in vastly lower steps like 8 or 4.</p> <p>Though sometimes they come along with a dedicated sampler like <code>lcm</code>, they may work in tandem with training-free samplers in general. For example, you can probably use any of <code>euler</code>, <code>dpmpp_2m</code>, and more on a model with a Hyper LoRA applied.</p>"},{"location":"Sampling/05_training_free_list/","title":"(Non-exhaustive) List of Training-Free Samplers","text":"<p>At the beginning of each section, you may find a table summarizing the samplers mentioned in the section.</p> Sampler Time Spent Order Converges Notes Name of the sampler How many times slower than <code>euler</code>. 1x=the same as <code>euler</code>, 2x=takes twice as long as <code>euler</code>, etc The order, as mentioned in #accuracy/control. Some technically support a range of orders, in that case, I'll include the default &amp; range. Yes (refines 1 image with more steps) / No (may change composition with more steps) Some notes about the sampler"},{"location":"Sampling/05_training_free_list/#explicit-runge-kutta-methods-euler-heun-and-beyond","title":"Explicit Runge-Kutta Methods: Euler, Heun, and Beyond","text":"Sampler Time Spent Order Converges Notes <code>euler</code> 1x 1 Yes Simplest and most inaccurate, makes soft lines &amp; blurs details. <code>euler_ancestral</code> 1x 1 No Like <code>euler</code> but divergent (adds noise), popular. <code>heun</code> 2x 2 Yes Can be thought of as the \"improved\" <code>euler</code> <code>bosh3</code> 3x 3 Yes 3rd order RK <code>rk4</code> 4x 4 Yes 4th order RK <code>dopri6</code> 6x 5 Yes 6 model calls/step is needed for order 5. <p><code>euler</code>, <code>heun</code>, and the rarer <code>fehlberg2</code>, <code>bosh3</code>, <code>rk4</code>, <code>dorpi6</code>, and more, all fall under the umbrella of explicit Runge-Kutta(RK) Methods for solving ODEs. They were developed way before any diffusion model, or even any modern computer, came to be.</p> <p>RK methods are singlestep, which means that the higher order ones take a while to run. <code>bosh3</code> for example takes 3 times longer than <code>euler</code> per step. Combined with the fact that diffusion DEs are stiff, this means that it's seldom worth using a high-order explicit RK method by itself, as it massively increases sampling time while netting you a very marginal gain in quality. Personally, I'd at most use <code>bosh3</code>, though even that's cutting it close. It's no wonder that you don't find some of these in popular UIs.</p> <p>Samplers developed down the road employ optimizations specific to diffusion equations, making them the better choice 90% of the time. </p> <p>Where Can I Find Higher Order RK Samplers?</p> <ul> <li>ComfyUI: ComfyUI-RK-Sampler or RES4LYF</li> <li>reForge: Settings-&gt;Sampler according to this</li> </ul>"},{"location":"Sampling/05_training_free_list/#early-days-of-diffusion-ddpm-ddim-plmspndm","title":"Early Days of Diffusion: DDPM, DDIM, PLMS(PNDM)","text":"Sampler Time Spent Order Converges Notes <code>ddpm</code> 1x 1 No The original diffusion sampler <code>ddim</code> 1x 1 Yes Converges faster than DDPM, trading a bit of quality <code>plms</code>=<code>pndm</code> 1x default 4 (up to 4) Yes LMS tailored for use in diffusion (uses Adams\u2013Bashforth under the hood) <p>DDPM was what started it all, applying diffusion models to image generation, achieving really high quality but requiring a thousand steps to generate a sample. </p> <p>Through adjustments to the diffusion equation, people arrived at DDIM, drastically reducing the number of steps required at the cost of a little quality.</p> <p>PNDM finds that classical methods don't work well with diffusion equations. So they design a new way to do it - pseudo numerical methods. They then tried many approaches and found that Pseudo Linear Multistep Method (PLMS) seemed the best, hence the other name.</p> <p>These 3, along with the classical ODE solvers <code>euler</code>, <code>heun</code>, and <code>LMS</code>, were the original samplers shipped with the release of the original stable diffusion.</p>"},{"location":"Sampling/05_training_free_list/#steady-improvements-deis-ipndm-dpm","title":"Steady Improvements: DEIS + iPNDM, DPM","text":"Sampler Time Spent Order Converges Notes <code>deis</code> 1x default 3 (up to 4) Yes <code>ipndm</code> 1x 4 Yes <code>ipndm</code> found empirically better than <code>ipndm_v</code> <code>ipndm_v</code> 1x 4 Yes <code>dpm_2</code> 2x 2 Yes <code>dpm_2_ancestral</code> 2x 2 No <code>dpm_adaptive</code> 3x default 3 (2 or 3) Yes ignores steps &amp; scheduler settings, runs until it stops itself <code>dpm_fast</code> 1x (averaged) between 1~3 Yes Uses DPM-Solver 3,2,1 such that the number of model calls = number of steps, effectively taking the same time as <code>euler</code> would in the same number of steps <p>DEIS and DPM independently came to the same conclusion: Diffusion equations are too stiff for classical high-order solvers to do well. They use a variety of techniques to remedy this.  Notably, they both solve a part of the equation exactly, removing any error associated with it while leaving the rest less stiff. This idea is so good in fact that many samplers down the road also do it.</p> <p>The DEIS paper also introduced \"improved PNDM\" (iPNDM). <code>ipndm_v</code> is the variable step version that should work better for diffusion, though they find empirically that <code>ipndm</code> performs better than <code>ipndm_v</code>.</p> <p>Differing Results With <code>ipndm_v</code></p> <p>In my personal tests in ComfyUI, for some reason, I find for the same exact parameters - prompt, seed, etc. - <code>ipndm_v</code> sometimes breaks (lots of artifacts) if you use <code>KSampler</code>, but not if you use <code>SamplerCustomAdvanced</code>. In fact, I've never gotten any image breakdown with <code>SamplerCustomAdvanced</code> + <code>ipndm_v</code>, unless at low steps where it hasn't converged. Bextoper has also noted that <code>ipndm_v</code> breaks similarly to <code>KSampler</code> in Forge.</p>"},{"location":"Sampling/05_training_free_list/#cascade-of-new-ideas-dpm-unipc-restart-res-gradient-estimation-er-sde-seeds","title":"Cascade of New Ideas: DPM++, UniPC, Restart, RES, Gradient Estimation, ER SDE, SEEDS","text":"Sampler Time Spent Order Converges Notes <code>dpmpp_2s_ancestral</code> 2x 2 No \"<code>dpmpp</code>\" as in \"DPM Plus Plus\" = \"DPM++\" <code>dpmpp_sde</code> 2x 2 No I think this is \"SDE-DPM-Solver++(2S)\" not found explicitly defined in the paper <code>dpmpp_2m</code> 1x 2 Yes <code>dpmpp_3m_sde</code> 1x 3 No <code>uni_pc</code> 1x 3 Yes Official repo <code>uni_pc_bh2</code> 1x 3 Yes Empirically found a little better than <code>uni_pc</code> in guided sampling <code>Restart</code> default 2x (varies) default 2 (varies) No Time Spent &amp; order depends on the underlying solver used (paper uses <code>heun</code>); Official repo <code>res_multistep</code> 1x 2 Yes The authors give a general way to define <code>res_singlestep</code> for any order <code>gradient_estimation</code> 1x 2 (?) Yes Uses 2 substeps, so I guess order 2? Not sure if the notion of order really applies... <code>seeds_2/3</code> 2/3x 2/3 No <code>er_sde</code> 1x default 3 (1-3) No Official repo <p>*(this nothing to do with StableCascade I just thought it's a cool title)</p> <p>Around this time, the idea of guidance took off, offering the ability to specify what image we want to generate, but also bringing new challenges to the table:</p> <ul> <li>High guidance makes the DE even stiffer, breaking high-order samplers</li> <li>High guidance knocks samples out of the training data range (train-test mismatch), creating unnatural images</li> </ul> <p>To address issues with high CFG, DPM++ adds 2 techniques (that were proposed in prior works by others already) to DPM:</p> <ul> <li>Switch from noise (eps, \\(\\epsilon\\)) prediction to data (\\(x_0\\)) prediction (which they show is better by a constant in Appendix B).</li> <li>The above also allows them to apply thresholding to push the sample back into training data range.</li> </ul> <p>Practical; Not a Technical Marvel</p> <p>The <code>dpmpp</code> family, especially <code>dpmpp_2m</code>, are one of the most widely used samplers alongside <code>euler_ancestral</code>. However, it was rejected by ICLR due to heavily relying on existing works, so it \"does not contain enough technical novelty.\"</p> <p>UniPC came soon after. Inspired by the predictor-corrector ODE methods, they develop UniC, a corrector that can be directly plugged after any existing sampler to increase its accuracy. As a byproduct, they derive UniP from the same equation as UniC, which is a predictor that can go up to any arbitrary order. The two combine to UniPC, achieving SOTA results using order=3.</p> <p>Restart doesn't actually introduce a new sampler, instead focusing on the discrepancy between trying to solve diffusion as an ODE (no noise injections) vs. an SDE (injects noise at every step). To get the best of both worlds, Restart proposes that rather than injecting noise at every step, let's do it in infrequent intervals. </p> *Visualization of ODE, SDE, and Restart taken from their [official repo](https://github.com/Newbeeer/diffusion_restart_sampling)* <p>RES identified an overlooked set of conditions that solvers must satisfy to achieve their claimed order (they find that <code>dpmpp</code> doesn't satisfy some of these, leading to worse-than-expected results). They then unify the equation for noise prediction and data prediction, making analysis easier. Finally, they pick coefficients that satisfy these additional conditions.</p> <p>Gradient Estimation finds that denoising can be interpreted as a form of gradient descent, and designs a sampler based on it.</p> <p>SEEDS rewrites a whole lot of equations so that more parts can be solved exactly or approximated more accurately. To make sure the equation stays true, a modified way of injecting noise is used. They derive SEEDS for both eps-pred and data-pred, though the former is very slow so ComfyUI includes only the latter.</p> <p>ER SDE models the diffusion process as an Extended Reverse-Time SDE and develops a solver based on that.</p>"},{"location":"Sampling/05_training_free_list/#out-of-reach-amed-dc-solver-among-others","title":"Out of Reach: AMED, DC-Solver, among Others","text":"<p>With so many samplers, it's no surprise that some have been left out of the party. This section includes sampling techniques that, while exist in literature or python code, are unavailable in popular UIs that are more accessible to non-coders. Since they're not widely available, discussion is also low so there will be many more that I simply don't know about and are missing from the list.</p> <p>Techniques Not Included: (in no particular order)</p> <ul> <li>UniC: As said before, you could in theory plug UniC after any sampler to achieve better accuracy. No UI lets you actually do that though to my knowledge.</li> </ul> <p>Samplers Not Included: (in no particular order)</p> <ul> <li>AMED</li> <li>DC-Solver</li> <li>GottaGoFast</li> <li>Era-Solver</li> <li>SciRE-Solver</li> <li>gDDIM</li> <li>Analytic-DPM</li> </ul>"},{"location":"Sampling/06_training_based_list/","title":"(Non-exhaustive) List of Training-Based Sampling Methods","text":"<p>UNDER CONSTRUCTION</p> <p>This section is incomplete and subject to massive changes. </p> <ul> <li><code>lcm</code>: Consistency Models defines a new training objective for the model - to learn the \"consistency function (CF),\" which is some function <code>f(x, t) = x_0</code> that takes the current noisy image and the time step, and outputs the origin - that is, the completely denoised image. LCM applies the idea to latent-based models (like stable diffusion).<ul> <li>Sampling in 1 step in practice doesn't yield great images. To fix this, they enable it to run multiple steps by re-injecting noise. The procedure is: 1st prediction, inject noise back, predict again, ... repeat.</li> </ul> </li> <li><code>turbo</code>: Paper authors use both adversarial loss and distillation loss to further train a model to allow it to generate images in a few steps.<ul> <li>Adversarial loss: A neural network (the \"discriminator\") is used to \"detect\" if the few-step-generated image is AI-generated; If the discriminator successfully detects it, the imagegen model is punished and forced to do better. The discriminator is also trained by us and not perfect. Only using adversarial loss eventually leads to the model to exploit the discriminator's defects that, while minimizing adversarial loss, results in horrible images.</li> <li>Distillation loss: During training, which requires the model being trained (the \"student\") to predict the noise, a different imagegen model (the \"teacher\") also predicts the noise along with it. The student is rewarded if what it predicts is close to what the teacher predicts. This helps ground the student model to generate sane images and not abuse the discriminator's imperfections.</li> </ul> </li> <li><code>lightning</code>: Basically improves upon the methods of <code>turbo</code>. <code>turbo</code> used a pre-trained encoder as the foundation for the discriminator, which introduced several problems like increased training hardware reqs, stuff like LoRAs being less compatible, etc. <code>lightning</code> directly uses the diffusion model's encoder as the discriminator's backbone instead.</li> <li><code>tcd</code>: Authors identify issues on why <code>lcm</code> can't make clear images with good detail, which is the accumulated errors due to practical limitations during training and sampling. To fix this they:<ul> <li>Update the training objective: They expand the definition of the original CF and arrive at the \"trajectory consistency function (TCF),\" <code>f(x, t, s) = x_s</code>. Compared to the original CF, TCF additionally takes an input <code>s</code>, and outputs the partially denoised image at timestep <code>s</code> (if <code>s = 0</code> then the output is the completely denoised image again).</li> <li>Update the sampling method: The new objective being TCD allows them to use stochastic sampling, which helps to correct accumulated errors.</li> <li>Several key ideas of TCD are highly similar to that of CTM and the two have a dispute. TCD has been accused of plagiarism by CTM's authors, with there being a response on the same post and TCD's code repo, with CTM's authors being dissatisfied with said response in this issue. I can't find anything that happened afterward.</li> </ul> </li> <li><code>hyper</code>: Takes ideas from CTM and DMD, and incorporates human feedback to train a low-step genning model and LoRA.</li> <li><code>dmd2</code>: DMD says rather than rewarding the student model for following the teacher model step by step, only reward the final result and ignore how the student does it. This did great but came with problems like high hardware reqs and the student following the teacher step by step anyway, that DMD2 fixes:<ul> <li>Use the Two-Time Scale Rule inspired by this paper, reducing hardware reqs as DMD2 no longer requires generating an entire dataset using the teacher to stabilize training.</li> <li>Additionally use a discriminator as well, which potentially allows the student to surpass the teacher.</li> <li>1 step generation still isn't great, so they extend it to support many-step generation like how <code>lcm</code> does it (predict, noise inject, repeat).</li> </ul> </li> <li><code>pcm</code>: Again inspired by consistency models, rather than predicting the completely denoised image, cut it into multiple intermediate steps and try predicting from one to the next. This also solves the train-test mismatch of what the model was trained for not matching how it's used in practice.</li> </ul>"},{"location":"Sampling/07_tldr_usage/","title":"Practical TL;DR of Samplers","text":""},{"location":"Sampling/07_tldr_usage/#iteration-speed-how-long-1-step-takes","title":"Iteration Speed (How Long 1 Step Takes)","text":"Relative Speed Samplers 1x <code>euler</code>, <code>dpmpp_2m</code> (and everything with <code>&lt;number&gt;m</code> in the name), most others 2x <code>heun</code>, <code>dpm_2</code>, <code>dpmpp_sde</code>, <code>seeds_2</code>, <code>dpmpp_2s_ancestral</code>, <code>sa_solver_pece</code> (and everything with <code>2s</code> in the name) 3x <code>heunpp</code>, <code>seeds_3</code>, <code>dpm_adaptive</code>, anything with <code>3s</code> in the name <p>For example, this means that in the same time that you run <code>euler</code> for 20 steps, it takes about the same time to run <code>dpm_2</code> for 10 steps.</p> <p><code>dpm_fast</code> is a bit special; This sampler switches between the <code>DPM-3</code>, <code>DPM-2</code>, and <code>DPM-1</code> algorithm in a way that the total amount of time spent is the same as <code>euler</code> if you set the same amount of steps. Each step may take differing amounts of time.</p>"},{"location":"Sampling/07_tldr_usage/#samplers-that-change-composition-as-you-increase-step-count","title":"Samplers That Change Composition As You Increase Step Count","text":"<ul> <li><code>ddpm</code> </li> <li><code>restart</code> </li> <li><code>seeds</code> </li> <li><code>sa_solver(_pece)</code></li> <li>Any sampler with <code>ancestral (a)</code> in the name</li> <li>Any sampler with <code>sde</code> in the name</li> </ul> <p>Generally, these samplers beat their non-composition-changing counterparts if you use higher step counts.</p>"},{"location":"Sampling/07_tldr_usage/#sdxl-what-works-at-various-step-counts","title":"SDXL - What Works at Various Step Counts","text":"Steps Works Well Usually Breaks 1-5 Training-based methods like <code>lcm</code>, <code>lightning</code>, <code>hyper</code> Everything else 6-15 Many (realistically, 6-10 steps will also leave very visible artifacts, but may occasionally be ok.) <code>ipndm(_v)</code>, <code>dpmpp_3m_sde</code>, <code>seeds_3</code> 16-30 Mostly everything <code>dpmpp_3m_sde</code> may still leave artifacts 30+ High-order methods like <code>ipndm(_v)</code> / Stochastic methods like <code>dpmpp_3m_sde</code> - <p>But <code>X</code> Worked/Broke For Me!</p> <p>These were tested on Amanatsu, a very stable eps-pred SDXL, on a few samples. You should see these only as general rules of thumb on what works in low / high step environments. For example, most SDXL v-pred models require a ton more steps with any sampler, 20+ for only decent results in my experience.</p>"},{"location":"Schedule/00_schedule/","title":"What is a Noise Schedule?","text":"<p>UNDER CONSTRUCTION</p> <p>This section is incomplete and subject to massive changes. </p> <p>What's The Difference Between Samplers and Schedulers?</p> <p>The sampler controls the way to remove that amount of noise. The scheduler controls how much noise to remove at each step.</p> <p>A scheduler is (a way to generate) a list of numbers that quantify the amount of noise in an image (usually denoted using sigma \\(\\sigma\\)). For generating images, \\(\\sigma\\) starts at a high value (very noisy image) and ends at a very low value (clear images). </p> <p>The actual values of the highest and lowest \\(\\sigma\\) (\\(\\sigma_\\text{max}, \\sigma_\\text{min}\\)) are different across models, which in turn affect the values that schedulers generate, but this isn't something you have to worry about as it's handled by your frontend (A1111, ComfyUI, forge, etc).</p> <p>For practical training reasons, \\(\\sigma_\\text{min}\\) actually may not be 0 (which would represent a clean, no-noise image). A 0 is often added to the end of the list to represent that there should be no noise in the final result.</p>"},{"location":"Schedule/01_schedule_list/","title":"(Non-exhaustive) List of Schedulers","text":"<ul> <li><code>normal</code>: Time uniform sampling that includes both <code>\u03c3_max</code> and <code>\u03c3_min</code><ul> <li>In practice, this may lead to having a final tiny step (<code>\u03c3_min</code> to 0) that doesn't behave well</li> </ul> </li> <li><code>karras</code>: The schedule introduced in EDM by Karras et al.</li> <li><code>exponential</code>: A schedule where the <code>\u03c3</code> follows an exponential curve</li> <li><code>sgm_uniform</code>: <code>normal</code>, except that <code>\u03c3_min</code> is not included<ul> <li>You can probably replace <code>normal</code> with this one in all situations</li> <li>AFAICT, the naming comes from StabilityAI's generative-models repository.</li> </ul> </li> <li><code>simple</code>: comfy wrote the simplest scheduler they could think of for fun. In practice, basically the same as <code>sgm_uniform</code>.</li> <li><code>ddim_uniform</code>: The schedule used in the original DDIM paper</li> <li><code>beta</code>: Paper authors found the model settles general composition in the early steps while ironing out small details in the later steps, with \"nothing\" really happening in the middle steps. They thus argue to use the beta distribution for generating a schedule, which would give more steps to both ends and leave a few steps in the middle.</li> <li><code>kl_optimal</code>: Introduced in AYS paper that theoretically would minimize the kl divergence.</li> <li><code>AYS</code> (Align Your Steps): Predefined lists of noise levels found through numerical optimization to be good for a specific model architecture.<ul> <li><code>SD1</code> / <code>SDXL</code> / <code>SVD</code>: Match this with the one your model is based on. </li> <li><code>AYS 11</code> / <code>AYS 32</code>: <code>AYS</code> tuned for 10 / 31 steps generation. If you select any step count besides 10 / 31, log-linear interpolation is used to create the undefined noise levels.</li> </ul> </li> <li><code>GITS</code> (Geometry-Inspired Time Scheduling): Paper authors examine different imagegen neural nets and found surprisingly similar generation trajectories across them. They exploit the structure of this trajectory, which is GITS.</li> </ul>"}]}