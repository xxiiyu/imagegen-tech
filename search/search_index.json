{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Preamble","text":"<p>Greetings. You may know me as <code>11yu</code> from various botmaking / imagegen discords.</p> <p>This guide is mostly made with local generation in mind, and is not a good introduction to imagegen in general. You can check out some #other guides for that. Instead, I'll be focusing more on:</p> <ul> <li>1-2 steps further into the technical aspects of what a usual imagegen guide may provide.<ul> <li>This is to compliment the multitude of other online guides that say \"Use X (if Y)\" with little to no helpful explanation.</li> <li>You might find the gory, even more technical details in sections marked <code>[DD]</code>, or a separate <code>Deep Dive</code> section at the end of a page, if it exists.</li> </ul> </li> <li>lesser-known tips, tricks, and technologies that may improve your imagegen process.</li> </ul> <p>Everything here should be taken with a mountain of salt, as in practice, many other variables could influence your results greatly. Also, this article is mostly written by a dumb person (me). You will likely come out of a section thinking: \"Interesting. This doesn't help me at all.\"</p> <p>With that in mind, let's dive in.</p> <ul> <li>Samplers</li> <li>Schedulers</li> <li>Guidance</li> </ul>"},{"location":"Appendix/other_guides/","title":"Other Guides","text":"<ul> <li>Bex's Stable Diffusion Tips and Tricks; and General Usage Guide for Illustrious Models</li> <li>Skelly's Necronomicon: Art Generating</li> <li>StatuoTW's Guide to Making Bots - #Generating AI Art, a Guide to your First AI Gens.</li> </ul>"},{"location":"Appendix/references/","title":"References","text":"<p>Other materials referenced in the making of this rentry, that weren't already embedded into their respective sections. In no particular order:</p> <ul> <li>sander.ai</li> <li>AI Summer</li> <li>huggingface's <code>diffusers</code></li> <li>Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices</li> <li>Diffusion Models: A Comprehensive Survey of Methods and Applications</li> <li>Efficient Diffusion Models: A Survey</li> <li>Complete guide to samplers in Stable Diffusion</li> <li>Loss Functions in Diffusion Models: A Comparative Study</li> <li>(Karras) Elucidating the Design Space of Diffusion-Based Generative Models</li> <li>(Song) Score-Based Generative Modeling through Stochastic Differential Equations</li> </ul>"},{"location":"Appendix/special_thanks/","title":"Special Thanks","text":"<p>My sincerest thank you, to all the people of this community. I would've never thought to make anything like this without yall.</p>"},{"location":"ComfyUI/00_comfyui_intro/","title":"Exploring Internals","text":"<p>ComfyUI is great, except when it's not. Documentation is especially lacking: If what you're looking for isn't some generic issue that has been answered 10 times before, you're most likely out of luck.</p> <p>This section hopes to alleviate some of that, by actually digging into the internals and explaining what the nodes do. Through better understanding comes better informed decisions, like when/why should X node be used in this situation.</p>"},{"location":"ComfyUI/00_comfyui_intro/#recommended-usage","title":"Recommended Usage","text":"<p>Though it is not done, I assume this section will quickly become messy and unorganized. I will try my best to make each item easily searchable.</p> <p>If you're looking for a specific node, I thus recommend not going through each page, but simply use the search function located at the top right of the website.</p>"},{"location":"ComfyUI/00_comfyui_intro/#methodology","title":"Methodology","text":"<p>I make extensive use of:</p> <ul> <li><code>Power Puter (rgthree)</code>: Allows you to run basically arbitrary python. I set the output type to <code>*</code> most of the time for digging purposes.</li> <li><code>Display Any (rgthree)</code>: Allows you to display basically anything.</li> <li><code>\ud83d\udd27 Debug Tensor Shape (comfyui-essentials)</code>: A convenience node for showing tensor shapes, though the functionality can technically be achieved with <code>Power Puter</code> too.</li> </ul> <p>If you want to dig into ComfyUI nodes yourself I highly recommend giving the above nodes a try. Obviously, this also requires you to know python.</p>"},{"location":"ComfyUI/00_comfyui_intro/#ramble","title":"Ramble","text":"<p>You can ignore this section, I just want to vent.</p> <p>There's nothing of substance here.</p> <p>...</p> <p>You're still here? Oh well. Enjoy seeing me crash out I guess.</p> <p>But seriously. How is it so bad.</p> <p>\"Hmm. I wonder what <code>ModelSamplingAuraFlow</code> does. ...ComfyUI Docs... Nope, no page on that... The other wiki? Nothing either. Let's check the internet.\"</p> <ul> <li><code>RunComfy</code>: <p>The <code>ModelSamplingAuraFlow</code> node is designed to enhance the sampling process of AI models, specifically tailored for the AuraFlow model. This node allows you to apply advanced sampling techniques to your model, which can significantly improve the quality and efficiency of the generated outputs. By leveraging the <code>patch_aura</code> method, it adjusts the sampling parameters to optimize the model's performance, ensuring smoother and more accurate results. This node is particularly beneficial for AI artists looking to fine-tune their models with precise control over the sampling process, ultimately leading to more refined and high-quality artistic outputs.</p> </li> </ul> <p>\"Oh, uhh... cooolll... doesn't help me understand a thing though. What about here?\"</p> <ul> <li><code>ComfyAI.run</code>: <p>The ModelSamplingAuraFlow ComfyUI node is a versatile tool designed for advanced users aiming to fine-tune models through dynamic sampling. It allows users to apply a shift parameter to models, catering to various AI model development and experimentation needs. This node can be accessed and run through ComfyUI, whether on a local setup or a cloud-based service, providing flexibility and scalability for different deployment scenarios.</p> </li> </ul> <p>\"Well... let's check somewhere else?\"</p> <ul> <li><code>InstaSD</code>: <p>The <code>ModelSamplingAuraFlow</code> node in ComfyUI is designed to perform specific sampling functions related to the AuraFlow model within the broader framework of ComfyUI's modular visual AI engine. This node enables users to integrate AuraFlow's sampling techniques into their custom workflows, benefiting from its unique approach to model sampling.</p> </li> </ul> <p>Like, I just can't.  </p> <p>Why do both official wikis only talk about the nodes that everyone already knows what they do?  </p> <p>Are the unofficial websites serious with these \"explanations\" they give out? Can they write nothing but flowery yet useless sentences? Do they collectively share 1 singular LLM braincell?  </p> <p>My god.</p>"},{"location":"ComfyUI/data_types/","title":"Data Types","text":"<p>Before looking at individual nodes, it's helpful to know how the data they work with looks like, and how they behave.</p> <p>Also refer to Comfy docs:</p> <ul> <li>Datatypes</li> <li>Images, Latents, and Masks</li> </ul>"},{"location":"ComfyUI/data_types/#basic-types","title":"Basic Types","text":"<code>int</code> <code>float</code> / <code>double</code> <code>string</code> <code>bool</code> Semantic Meaning Whole numbers like <code>12345</code> Decimals like <code>3.14</code> Text like <code>\"I like trains\"</code> True or False <p>The most complex out of these is <code>float</code> - besides being a specific type, it also encompasses the idea of the \"floating point format/number\", which is a smart scheme to store a very wide range of numbers with a fixed bit budget.</p> <p>Usually, <code>float</code> actually means a 32-bit floating point number, while <code>double</code> means a 64-bit floating point number. You may also see some people / programming languages use other naming, e.g. <code>float16</code> <code>float32</code> <code>float64</code> where the number is how many bits.</p> <p>Floating Point Numbers</p> <p>A number stored in the floating point format is split into 2 parts, the \"mantissa\" and the \"exponent,\" in base 2 because computers. For demonstration, I'll show what that may look like using our every day normal numbers (in other words, base 10): \\(12345=\\underbrace{1.2345}_{\\text{mantissa}}\\times\\underbrace{10}_{\\text{base}}\\!\\!\\!\\!\\!\\!^{\\overbrace{4}^\\text{exponent}}\\)  Assume we have a fixed 32-bit budget. To represent massive / tiny numbers, you can allocate more of those 32 bits to the exponent, giving it greater range at the cost of accuracy (usually a worthwhile trade).</p> <p>JavaScript, the programming language powering ComfyUI (and most of the internet's) interactivity, uses <code>double</code> to store all numbers.  This leads to potentially surprising behavior. For example, a <code>double</code> starts to lose the ability to distinguish between \\(n\\) and \\(n+1\\) at \\(n=2^{53}=9007199254740992.\\) To see this in action, take a <code>Int (utils &gt; primitive)</code> node and try inputting <code>9007199254740993</code>. Alternatively, input <code>9007199254740992</code> and try to increase it by pressing the arrow.</p>"},{"location":"ComfyUI/data_types/#tensors","title":"Tensors","text":"<p>Tensors probably won't be the direct input or output of nodes, but understanding them will nevertheless prove immensely helpful imo.</p> 0D Tensor 1D Tensor 2D Tensor 3D Tensor 4D Tensor Also known as scaler vector matrix Explanation 1 number a list of numbers a grid of numbers a cube of numbers ComfyUI example <code>seed</code>, <code>cfg</code> <code>sigmas</code> (represents noise levels) pooled <code>conditioning</code> raw <code>conditioning</code> <code>image</code>, <code>latent</code> <p>In other words, a tensor is simply a multi-dimensional array of numbers (including 0D, a single number).</p>"},{"location":"ComfyUI/data_types/#images-latents-and-masks","title":"Images, Latents and Masks","text":"<p><code>image</code> and <code>latent</code> are both 4D tensors, while <code>mask</code> is a 3D tensor. The semantic meanings of the dimensions are:</p> <ul> <li><code>N (batch size)</code>: How many images/latents there are.</li> <li><code>C (channels)</code>: How many channels. For example, an image may have 3 channels, RGB.</li> <li><code>H (height)</code></li> <li><code>W (width)</code></li> </ul> <p><code>latent</code>s are in the \"channel first\" format - <code>NCHW</code>, meaning the first dimension is <code>N</code>, the second is <code>C</code>, the third is <code>H</code>, and the last is <code>W</code>.</p> <p><code>image</code>s are in \"channel last\" - <code>NHWC</code>.</p> <p><code>mask</code> doesn't have the channel dimension - <code>NHW</code>.</p>"},{"location":"ComfyUI/data_types/#frequency","title":"\"Frequency\"","text":"<p>For something more familiar like audio, the frequency is the number of vibrations with respect to time. Higher frequency = more vibrations = sounds like higher pitch.</p> <p>For <code>image</code>s (and by extension, <code>latent</code>s), it's the change in pixel values with respect to distance (X Y coordinates). </p> <ul> <li>High frequency = rapid change in color/brightness/etc in a small area; Corresponds to fine details.</li> <li>Low frequency = gradual change in color/brightness/etc in a large area; Corresponds to image structure.</li> </ul>"},{"location":"ComfyUI/data_types/#common-errors","title":"Common Errors","text":"<p>KSampler - Input type (double) and bias type (float) should be the same:</p> <ul> <li>Try switching your Preview method from <code>TAESD</code> to something else. </li> </ul>"},{"location":"ComfyUI/model_patches/","title":"Model Patches","text":"<p>This page is roughly for nodes that input and output a <code>model</code>, modifying said model in some way (like adjusting the cfg function, noise schedule, etc.)</p> <p>At the beginning of each section you can find a summary table, formatted like below:</p> Aspect Description Applicable To Which models work with the node in theory. In practice you can try whatever, though results will vary. Purpose Why / When to use this node, at least in theory. Notes Some things that I feel are important"},{"location":"ComfyUI/model_patches/#modelsamplingdiscrete","title":"ModelSamplingDiscrete","text":"Aspect Description Applicable To Diffusion Purpose Manually change prediction type. Notes You only need this node if the model file doesn't contain metadata about which type to use; If it does then comfy will detect it and this node serves no purpose. <ul> <li><code>sampling</code>: Match this with what the model you're using was trained on.<ul> <li><code>eps</code>: Predict the noise to remove. Also called \\(\\epsilon\\) (epsilon) prediction.</li> <li><code>v_prediction</code>: Predict the velocity (a mix between noise and data; not to be confused with flow matching's velocity).</li> <li><code>lcm</code>: Latent Consistency Models mode. Intended for #distill models / loras that use ideas from LCM, like SDXL Lightning, Hyper, DMD2, etc. Not required per se, but using this follows the theory and probably improves the quality.</li> <li><code>x_0</code>: Predict the data (clean image). Also called \\(x_0\\) prediction.</li> <li><code>img_to_img</code>: I'd guess for Lotus based on when this was added and the commit message adding it.</li> </ul> </li> <li><code>zsnr: true</code>: Force a Zero-Terminal Signal-to-Noise Ratio schedule. See #current schedules are bad for why this is important. Intended for <code>v_prediction</code>.</li> </ul> <p>Details</p> <p><code>sampling: eps, x0, v_prediction</code> are mathematically connected through these formulae:</p> <p>\\(x_\\theta=\\frac1{\\alpha_t}(x_t-\\sigma_t\\epsilon_\\theta)=\\alpha_tx_t-\\sigma_tv_\\theta\\)</p> <p>Where \\(\\alpha, \\sigma\\) are numbers relating to the noise schedule, \\(x_\\theta, \\epsilon_\\theta, v_\\theta\\) are the various prediction types, and \\(x_t\\) is the noise-image-mix currently being diffused.  </p> <p>This means in theory, you can run an eps-pred model as a v-pred model. In practice, it's best that you use the type that the model was trained with.</p> <ul> <li>The model we train already isn't perfect, and it's best to stick to what it was trained with lest it performs worse (train-inference mismatch).</li> <li>In certain situations, \\(\\alpha_t, \\sigma_t\\) may become 0 / infinite, breaking down the mathematical connection.</li> </ul>"},{"location":"ComfyUI/model_patches/#modelsamplingsd3","title":"ModelSamplingSD3","text":"Aspect Description Applicable To Flow Matching Purpose Quality enhancement (improve composition). Notes Sets \\(\\sigma_\\text{max}\\) and \\(\\sigma_\\text{min}\\) to 1 and 0, respectively, as is expected for flow matching. This unfortunately means you can't easily apply <code>shift</code> to diffusion models, even if in theory you can. <ul> <li><code>shift</code>: Higher <code>shift</code> biases the model to spend more time on higher noise levels, theoretically improving composition while removing detail.<ul> <li><code>1</code> disables shift.</li> <li><code>3</code> is the default determined by the SD3 team through human preference testing on 1024x1024. <code>6</code> was a close second.</li> </ul> </li> </ul> <p>Motivation: SD3 realized that using the same scheduler for different resolution images is a bad idea.</p> <p>Intuition: You start with random noise, the models pick up on its patterns to make stuff like shapes, details, etc. Higher resolution = more pixels = too many patterns = too many local details + bad composition.</p> <p>Details</p> <p>To \"bias models to certain noise levels,\" <code>shift</code> re-maps timesteps. Specifically: $$ t_\\text{new}=\\frac{\\text{shift}\\times t_\\text{old}}{1+(\\text{shift}-1)\\times t_\\text{old}} $$</p>"},{"location":"ComfyUI/model_patches/#modelsamplingauraflow","title":"ModelSamplingAuraFlow","text":"<p>Same as <code>ModelSamplingSD3</code>.</p> <p>Details</p> <p>The two only differ in how they make the model represent <code>timestep</code>s, however, I don't think comfy explicitly uses those anywhere, or if it does, it's always as a percentage anyway, so it doesn't matter. Specifically, due to how the theory evolved, there are 2 conventions: represent <code>timestep</code> from 0 to 1 or from 0 to 1000. <code>ModelSamplingAuraFlow</code> uses the former while <code>ModelSamplingSD3</code> uses the latter.</p>"},{"location":"ComfyUI/model_patches/#differentialdiffusion","title":"DifferentialDiffusion","text":"Aspect Description Applicable To All models Purpose Improve inpainting (Improve blending at mask edges). <p>An inpaint mask, which may contain values ranging from 0 to 1, by default results in a binary decision of \"this pixel should(n't) be inpainted.\" </p> <p><code>DifferentialDiffusion</code> enables models to utilize the full range of mask values, using them as \"how much change should be applied to this pixel.\"</p> <p>Theoretically improves blending at borders / reduces seams.</p>"},{"location":"ComfyUI/model_patches/#freeu-freeu_v2","title":"FreeU / FreeU_V2","text":"Aspect Description Applicable To UNets (<code>sd1.5</code>, <code>sdxl</code>) Purpose Quality enhancement <ul> <li><code>s1, s2</code>: Skip-connection attenuation scale. Higher = suppress skip-connection outputs. Supposedly decreases unnatural details.</li> <li><code>b1, b2</code>: Backbone strengthening scale. Higher = amplify backbone outputs. Supposedly improves denoising ability.</li> </ul> <p>YMMV. Probably have to do an exhaustive search to find the best parameters. </p> <p>Details</p> <p>\"Free lunch for diffusion UNets that improve quality with no cost.\" -authors</p> <p>UNet outputs comprise two parts: backbone and skip-connections. Authors argue that the backbone is more important, and at the same time, skip-connections may introduce unnatural high-frequency features.</p>"},{"location":"ComfyUI/model_patches/#fresca","title":"FreSca","text":"Aspect Description Applicable To All models Purpose Quality enhancement <p>Apply independent scaling to high-frequency (details) and low-frequency (structure) features.</p> <p>Paraphrasing:</p> <ul> <li><code>scale_high: &gt;1, scale_low: 1</code>: \"fine-detail enhancement\"</li> <li><code>scale_high: &lt;1, scale_low: &gt;1</code>: \"smoothing\"</li> <li><code>scale_high, scale_low: 1</code>: disables functionality</li> </ul> <p><code>freq_cutoff</code>: Higher = more stuff is considered low frequency.</p>"},{"location":"ComfyUI/model_patches/#cfg-mods","title":"CFG Mods","text":"<p>Overshooting - usually occuring due to a cfg guiding effect that is too strong - is when an image during sampling is guided outside of what the model has been trained on, so the model panics and starts making poor predictions.</p> <p>Overshooting leads to low-quality images, with common issues including extreme saturation, color distortion, and a general lack of detail. I'll collective call these artifacts cfg burn.</p> <p>Details</p> <p>For this section, I'll be assuming that the normal cfg function looks like the following: $$ x_\\text{cfg}=x_n+w(x_p-x_n) $$ Where \\(x_\\text{cfg}\\) is the normal cfg denoised result, \\(w\\) is cfg, \\(x_p\\) is positive prediction, and \\(x_n\\) is negative prediction. When these symbols are used later that's what they mean.</p>"},{"location":"ComfyUI/model_patches/#rescalecfg","title":"RescaleCFG","text":"Aspect Content Applicable To All models (originally intended for v-pred) Purpose Mitigate overshooting. Notes Does nothing if cfg is 1. <p>This takes the original cfg denoised result \\(x_\\text{cfg},\\) and scale it down into \\(x_\\text{scaled}\\) to prevent overshoot. The two are then averaged together and that becomes the final model output. </p> <p>The authors chose to not directly use \\(x_\\text{scaled}\\) as the final model output, because \"the generated images are overly plain.\" </p> <ul> <li><code>multiplier</code>: cfg rescale multiplier \\(\\phi.\\) Higher = use more of the rescaled result to make the mix.<ul> <li>For example, <code>0.7</code> means that the final output is 70% \\(x_\\text{scaled}\\) and 30% \\(x_\\text{cfg}.\\)</li> </ul> </li> </ul> <p>Compared to simply lowering cfg, this ideally preserves the strong guiding effect of high cfg, but tones it down dynamically during sampling if it's about to overshoot.</p> <p>Deatils</p> <p>Changes the cfg function to the following: $$ x_\\text{final}=\\phi\\times\\frac{\\text{std}(x_p)}{\\text{std}(x_\\text{cfg})}\\times x_\\text{cfg}+(1-\\phi)\\times x_\\text{cfg} $$ Where \\(x_\\text{final}\\) is the final output and \\(\\text{std}\\) is the standard deviation.</p> <p>Motivation: \\(x_\\text{cfg}\\) may become too big, especially with high cfg and/or opposite pointing \\(x_p,x_n,\\) making the model overshoot. </p> <p>Fix: Match the size of \\(x_\\text{cfg}\\) to that of \\(x_p.\\) </p>"},{"location":"ComfyUI/model_patches/#renormcfg","title":"RenormCFG","text":"Aspect Content Applicable To All models (originally from Lumina Image 2.0) Purpose Mitigate overshooting. Notes Does nothing if cfg is 1. <ul> <li><code>cfg_trunc</code>: during sampling, set cfg to 1 at high noise levels (when <code>sigma &gt; cfg_trunc</code>), otherwise sample normally. <ul> <li>This doesn't trigger comfy's fast sampling optimization (where if you set <code>cfg = 1</code>, comfy recognizes it can skip calculating the negative, resulting in 2x speed).</li> <li><code>100</code>: Flow models' \\(\\sigma\\) range from 1 to 0, so this is intended as the \"disable this function\" big value. You can connect a <code>Float (utils &gt; primitive)</code> to it and set it bigger if you need it.</li> </ul> </li> <li><code>renorm_cfg</code>: cfg renorm multiplier \\(\\rho.\\) <ul> <li><code>0</code>: Disable.</li> <li><code>&lt;1</code>: Image becomes very washed very fast.</li> <li><code>&gt;1</code>: Does renorm less aggressively. Higher = less effect = closer to no renorm.</li> </ul> </li> </ul> <p>Deatils</p> <p>Actually combines 2 techniques, CFG-Renormalization and CFG-Truncation.</p> <p>CFG-Renorm, like CFG-Rescale, also seeks to mitigate overshoot, and also uses \\(x_p\\) as a guide. CFG-Renorm just uses this formula instead: $$ x_\\text{final}=\\rho\\times\\frac{||x_p||}{||x_\\text{cfg}||}\\times x_\\text{cfg} $$ Where \\(||...||\\) is the L2 norm, in other words \\(\\sqrt{x_1^2+x_2^2+x_3^2+...}\\) where the \\(x_1,x_2\\) etc are the elements of said tensor.</p> <p>CFG-Trunc skips using cfg at later steps because some other research found it affects little. ...which would mean that comfy's nodes and the research it's based on do opposite things (comfy skips at early steps)? Probably just me being dumb and overlooking something, but idk.</p>"},{"location":"ComfyUI/model_patches/#cfgnorm","title":"CFGNorm","text":"Aspect Content Applicable To All models (originally from Hidream E1) Purpose Mitigate overshooting. Notes Does nothing if cfg is 1. <p>Similar to (but no the same as) CFG-Renormalization.</p>"},{"location":"ComfyUI/model_patches/#adaptive-projected-guidance","title":"Adaptive Projected Guidance","text":"Aspect Content Applicable To All models. Purpose Mitigate overshooting. <ul> <li><code>eta</code>: Scaling of parallel component. <ul> <li><code>&gt;1</code>: Higher saturation.</li> <li><code>0~1</code>: Intended usage.</li> <li><code>&lt;0</code>: Probably don't (moves away from positive).</li> </ul> </li> <li><code>renorm_threshold</code>: Higher = less aggresive renorming.</li> <li><code>momentum</code>: Influence of previous steps on current step<ul> <li><code>&lt;0</code>: Intended usage. Push the model away from the prior prediction direction.</li> </ul> </li> </ul> <p>Some recommend <code>eta: 1, norm_threshold: 20, momentum: 0</code> to replace <code>RescaleCFG</code> for vpred.</p> <p>Details</p> <p>APG incorporates 3 ideas:</p> <ol> <li> <p>Supress parallel component: Specifically, decompose \\(D=w(x_p-x_n)\\) into two components, one parallel and one perpendicular to the positive prediction \\(x_p:\\)</p> <ul> <li>Parallel: Seems to be responsible for saturation.</li> <li>Perpendicular: Seems to be responsible for image quality improvements.  </li> </ul> <p>Thus, it makes sense to scale the parallel component by some factor \\(\\eta &lt;1.\\)</p> </li> <li> <p>Renormalize CFG: It's renorm again but different (tm). APG constrains \\(D\\) to have a size of at most <code>renorm_threshold</code>.</p> </li> <li> <p>Reverse momentum: Add a negative momentum term that pushes the model away from previously taken directions and encourages the model to focus more on the current update direction.</p> </li> </ol>"},{"location":"ComfyUI/model_patches/#tangential-damping-cfg","title":"Tangential Damping CFG","text":"Aspect Content Applicable To All models. Purpose Mitigate overshooting. <p>Intuition: During sampling, the negative prediction still points towards \"natural images;\" Trying to move away from it thus may have unintended consequences.</p> <ul> <li>Example: You negative <code>chibi</code>. However, <code>chibi</code> still lies in the overall space of anime images. So, besides removing <code>chibi</code>, you're actually also subtly telling the model to move away from anime images in general. Oops.</li> </ul> <p>Fix: Remove the part of the negative prediction that points towards natural images, so when you move away from it, you don't move away from natural images. </p> <p>Details</p> <p>Oversimplified, the idea is that a model, in learning how to transform noise to noise-image mixes to good images, also implicitly learns how to transform noise-image mixes to unnatural images that don't appear in training data. (proven through complex math).</p> <p>The component to remove from \\(x_n\\) is approximated using said information and the SVD algorithm.</p>"},{"location":"ComfyUI/model_patches/#self-attention-guidance","title":"Self-Attention Guidance","text":"Aspect Content Applicable To All models Purpose Quality enhancement (clarity / remove blur and melting color blobs) Notes Reduce your cfg accordingly. Increases generation time. <ul> <li><code>scale</code>: how much the SAG results influence things. Higher = more effect.</li> <li><code>blur_sigma</code>: the std of the Gaussian it uses for blur guidance. Higher = more aggressive blurring.</li> </ul> <p>In essence, SAG does the following:</p> <ul> <li>Identify high information (thus, difficult to diffuse) regions through the model's self-attention map.</li> <li>Blur this part with Gaussian noise, then predict with the model, resulting in a prediction based on blurry inputs.</li> <li>Do the usual cfg sampling on the original unblurred image. Then, additionally, move away from the blurry input prediction by <code>scale</code>.</li> </ul> <p>This should help most in regions where the model is very unconfident about what to put there, e.g., melting backgrounds. It probably doesn't help in regions where the model is confident, whether confidently right or confidently wrong.</p>"},{"location":"ComfyUI/model_patches/#perturbedattentionguidance","title":"PerturbedAttentionGuidance","text":"Aspect Content Applicable To All models Purpose Quality enhancement (remove degraded predictions) Notes Reduce your cfg accordingly: <code>PAG_scale + new_cfg = original_cfg</code> should be a good rule of thumb. Increases generation time. <p>Sequel to <code>Self-Attention Guidance</code>. They do different things, so you can use both at once.</p> <p>In essence, PAG does the following:</p> <ul> <li>Make a perturbed model by removing its self-attention and replacing it with an identity matrix. Have it predict on the image.</li> <li>Do the usual cfg sampling on the original image. Then, additionally, move away from the perturbed model prediction by <code>scale</code>.</li> </ul>"},{"location":"ComfyUI/model_patches/#mahiro-is-so-cute-that-she-deserves-a-better-guidance-function","title":"Mahiro is so cute that she deserves a better guidance function!! (\u3002\u30fb\u03c9\u30fb\u3002)","text":"Aspect Content Applicable To All models Purpose Quality enhancement (Better alignment to positive) <p>Motivation: normal cfg's negative prompt implicitly has the effect of making the model pay attention to the polar opposite of said negative.</p> <p>Fix: When applicable, pay more attention to the positive and less to the negative.</p> <p>Details</p> <p>Specifically:</p> <ol> <li>Calculate \"positive leap,\" which is \\(\\text{cfg}\\times x_p.\\) In the same vein, calculate \"negative leap.\"<ul> <li>A \"leap\" semantically means \"following positive/negative.\" cfg means \"follow positive as well as move away from negative.\"</li> </ul> </li> <li>Merge the positive leap and cfg, then calculate the similarity between that and the negative leap.<ul> <li>High similarity: The leap was bad, trust cfg more.</li> <li>Low similarity: The leap was good, trust the leap more.</li> </ul> </li> </ol> <p>(Find the quick read in the link, id <code>2024-1208.1</code>. Find a graph here comparing normal cfg and mahiro.)</p>"},{"location":"ComfyUI/model_patches/#appendix","title":"Appendix","text":""},{"location":"ComfyUI/model_patches/#clarifying-rescale-and-renorms","title":"Clarifying Rescale and Renorms","text":"<p>Ultimately, all aforementioned rescale/renorm methods aim to prevent model overshooting by reducing the \"size\" of intermediate outputs during the sampling process.</p> <p>The difference lies in how they measure the size, and which intermediate output they apply resizing to:</p> <code>RescaleCFG</code> <code>RenormCFG</code> <code>CFGNorm</code> <code>Adaptive Projected Guidance</code> Size measure standard deviation l2 norm l2 norm l2 norm Resizing applied to model cfg output model cfg output intermediate sample specific component of model cfg output Notes Final output is a mix of the unscaled and scaled output Only uses the channel dimension to calculate size <ul> <li>\"model cfg output\" is \\(x_\\text{cfg}=x_\\text{negative}+\\text{cfg}\\times(x_\\text{positive}-x_\\text{negative}).\\) </li> <li>\"intermediate sample\" is the noise-image-mix during sampling. <ul> <li>For example, using a noise-prediction model, you'd repeatedly do \\(x_\\text{next}=x_\\text{prev}-x_\\text{cfg}\\) until \\(x_\\text{next}\\) becomes a clean image. \\(x_\\text{next}\\) is what <code>CFGNorm</code> resizes.</li> </ul> </li> <li><code>Adaptive Projected Guidance</code> decomposes \\(\\text{cfg}\\times(x_\\text{positive}-x_\\text{negative})\\) further and resizes a part of that.</li> </ul> <p>What implications does this have in practice? IDFK, experiment ig.</p>"},{"location":"Guidance/00_guidance/","title":"What is Guidance?","text":"<p>In early diffusion models, generation was basically rolling dice (seeds). Assume you train a model on a dataset of elves and bunnies. While you can ask it to generate an image, you can't tell it to specifically generate an elf or a bunny. The only thing you can do is keep changing seeds, until one just so happen to appear.</p> <p>Guidance is the mechanism that solves this. It is the umbrella term for the techniques used to steer the model's generating process toward a specific target.</p> <p>Guidance can take many forms, from text prompts, to other input images, a hypernetwork such as ControlNet, and more.</p> <ul> <li>Classifier-Free Guidance</li> </ul>"},{"location":"Guidance/01_cg/","title":"Classifier Guidance","text":"<p>Classifier Guidance, like the name suggests, uses a classifier model to guide the diffusion model to generate our desired images. </p> <p>Note that an off-the-shelf classifier would not work out of the box, since those are trained on clean images, but diffusion models work with noisy images. </p> <p>One way to fix this is to utilize our diffusion model. Specifically, feed it the current noisy image \\(x_t\\) and try make it directly predict the clean image \\(\\hat x_0.\\) At high noise, \\(x_t\\) isn't very informative and the model would effectively try to predict the \"average image\" (of the entire training data), which when visualized looks like a very blurry image. </p> <p>However, classifiers are usually more robust to blurriness than noisyness, so we can feed this blurry prediction \\(\\hat x_0\\) into the classifier, to obtain the Classifier Guidance.</p> <p> ^ Demonstration of Classifier Guidance by sander.ai, where \\(\\gamma\\) is the guidance scale.</p> <p>And checkout sander.ai's blog on Classifier Guidance for a more thorough explanation.</p> <p>Still, the reliance on an external model isn't very appealing. That's why Classifier Guidance has largely been superseded by Classifier-Free Guidance (CFG), which does not need extra models other than the diffusion model itself to do guided generation.</p>"},{"location":"Guidance/CFG/00_cfg/","title":"What is CFG?","text":"<p>CFG in Flux</p> <p>Flux was not trained with CFG, thus nothing in this section applies. The \"Guidance\" value you can provide to flux is not CFG.</p> <p>It's simplest to understand how CFG works exactly by directly looking at the equation</p> \\[ x_\\text{cfg} = x_\\text{uncond} + \\omega(x_\\text{cond} - x_\\text{uncond}) \\] <ul> <li>\\(x_\\text{uncond}:\\) the model prediction without conditioning</li> <li>\\(x_\\text{cond}:\\) the model prediction with conditioning</li> <li>\\(\\omega:\\) CFG scale</li> <li>\\(x_\\text{cfg}:\\) result image</li> </ul> <p>For brevity, I'll be referring to \\(x_\\text{uncond}\\) and \\(x_\\text{cond}\\) as <code>uncond</code> and <code>cond</code> respectively. You can imagine that:</p> <ul> <li><code>uncond</code> is what the model thinks it should do.</li> <li><code>cond</code> is what the model thinks it should do, given our guidance.</li> </ul> <p>Let's play with some CFG numbers:</p> cfg <code>cfg_result</code> effect 0.1 <code>uncond*0.9 + cond*0.1</code> The effect that our guidance has is pretty weak. 90% of the generation process is still decided by <code>uncond</code>. 0.5 <code>uncond*0.5 + cond*0.5</code> The strength of our guidance is on even footing with the unguided conditioning. 1 <code>cond</code> The <code>uncond</code> cancels out, leaving us with only <code>cond</code>. The generation process is entirely decided by our guidance. 2 <code>2*cond - uncond</code> The model actively moves away from <code>uncond</code>, while the effect that our guidance has increases even more. <p>When implementing CFG in practice, people also noticed what we found here - namely that when <code>cfg &gt; 1</code>, the model moves away from <code>uncond</code>. Then, couldn't we use <code>uncond</code> as some sort of opposite guidance - \"Do anything but this?\" Yes! This is what became negative prompts.</p> <p>Now with that in mind, let's rewrite the equation in more familiar terms: <pre><code>denoised_image = negative + (positive - negative) * cfg\n</code></pre> Where you can imagine <code>positive</code> and <code>negative</code> as the representation of the positive and negative prompts respectively that the model understands. Armed with this knowledge, let's reiterate what happens with prompts at various CFG levels:</p> <ul> <li><code>cfg &lt; 1</code>: Negative prompts would behave like positive prompts.</li> <li><code>cfg = 1</code>: Negative prompts have no effect. </li> <li><code>cfg &gt; 1</code>: The model will actively avoid generating anything in the negative prompt.</li> </ul> <p>I also want to note something special about <code>cfg = 1</code> - that is, the negative prompt having no effect. Couldn't we skip calculating <code>negative</code> entirely then? Yep. ComfyUI does this, which is why you'll see 2x iteration speed if you set <code>cfg = 1</code>.</p> Differences in Notation <p>Some works prefer to base \\(x_\\text{cfg}\\) on \\(x_\\text{cond}\\) instead, resulting in this equivalent setup:</p> \\[ x_\\text{cfg}=x_\\text{cond}+\\omega'(x_\\text{cond}-x_\\text{uncond}) \\] <p>You can check that they're equivalent by plugging in \\(\\omega'=\\omega-1.\\)</p> <p><code>k-diffusion</code>, and by extension most popular UIs like Forge and ComfyUI, use the convention shown in the main text. (the one that uses \\(\\omega\\) rather than \\(\\omega'\\))</p>"},{"location":"Guidance/CFG/cfgpp/","title":"CFG++","text":"<p>Rescales CFG to work in the range of 0 to 1 making it work better at low guidance scales. It also makes the diffusing process smoother, and may lead to reduced artifacts. One usually accesses these by choosing predefined samplers, i.e. in ComfyUI <code>euler_ancestral_cfg_pp</code> is <code>euler_ancestral</code> using CFG++.</p> <p>In my and Bex's personal tests:</p> <ul> <li>CFG++ samplers give eps-pred models a very good color range, rivaling that of what v-pred models claim to achieve. </li> <li>Using CFG++ to inpaint at high steps breaks the image for reasons unknown.</li> </ul> <p>Note that in ComfyUI, the <code>_cfg_pp</code> samplers in <code>KSampler</code> are alt implementations where you instead simply want to divide the CFG you're used to by 2. For example, if you usually run <code>euler_ancestral</code> at CFG=7, you'd run <code>euler_ancestral_cfg_pp</code> at CFG=3.5. In practice, the reasonable range I find for these is CFG between <code>[0.7, 3]</code>, and <code>1.6</code> is a sane default to start with.</p>"},{"location":"Models/00_models/","title":"Introduction To Generative Models","text":"<p>A generative model learns to create new data similar to the training data. This is in contrast to a discriminative model which learns to look at a new data point and predict a label or number. For images, you can imagine that:</p> <ul> <li>generative model: learns to make new images</li> <li>discriminative model: learns to classify given images as cat or dog</li> </ul> <p>In modern day, generative models are usually deep neural networks, including GANs, VAEs, Language Models, and of course diffusion models.</p>"},{"location":"Models/00_models/#what-does-generating-mean","title":"What Does \"Generating\" Mean?","text":"<p>\"Generating\" new data, like images, is formalized as sampling from a probability distribution \\(p_\\text{data}\\). </p> <ul> <li>A probability distribution assigns probabilities to a set of specific objects. For example, a probability distribution for cat images \\(p_\\text{Cat}\\) would assign a high probability to cats, but a low probability to dogs, humans, plants, and everything else.</li> <li>Sampling means generating a specific object from that distribution according to its assigned probabilities. In the case of \\(p_\\text{Cat}\\), sampling would most often produce an image of a cat because it has a high probability, while images of dogs or humans would only appear very rarely, if ever.</li> </ul> Illustrative Example - Dice <p>Imagine rolling a fair die. The result would be a number 1-6, all with equal chance. The probability distribution of the dice roll \\(p_\\text{dice}\\) could thus be described as the following table:</p> \\(\\text{roll}\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) \\(p_\\text{dice}(\\text{roll})\\) \\(\\frac16\\) \\(\\frac16\\) \\(\\frac16\\) \\(\\frac16\\) \\(\\frac16\\) \\(\\frac16\\) Illustrative Example - Images <p>Imagine 2x2 images with 1 grayscale channel, where the channel value goes from 0-255. Each image can be thought of as a \\(2\\times2\\times1\\) list of numbers. For example, this could be a black and white image where the top left pixel has brightness 100, bottom right 200, and the rest completely dark:</p> <p>\\(\\begin{bmatrix}[100] &amp; [0] \\\\ [0] &amp; [200]\\end{bmatrix}\\)</p> <p>Assigning each image with a probability creates a probability distribution for these grayscale images:</p> \\(\\text{Image}\\) \\(\\begin{bmatrix}[100] &amp; [0] \\\\ [0] &amp; [200]\\end{bmatrix}\\) \\(\\begin{bmatrix}[42] &amp; [0] \\\\ [0] &amp; [42]\\end{bmatrix}\\) ... \\(p_\\text{data}(\\text{Image})\\) \\(\\frac1{1234}\\) \\(\\frac{11}{1337}\\) ... <p>Real world images are obviously more complex, having dynamic widths \\(W\\) and heights \\(H\\), with multiple color channels like RGB. This results in \\(W\\times H\\times 3\\) list of numbers. However, the principles stay fundamentally the same. </p>"},{"location":"Models/00_models/#why-modeling-p_textdata-is-a-problem","title":"Why Modeling \\(p_\\text{data}\\) is a Problem","text":"<p>Great! So, just train a neural network to learn \\(p_\\text{data},\\) right? Well... it's not so easy.</p> <p>First, the how. The modern solution is to train a model to take in Gaussian noise and output a realistic image. Since the Gaussian is very simple and easy to make in infinite supply, an endless stream of realistic images can be created by feeding the noise into said model. Neat!</p> <p>Second, the challenge: It's extremely difficult to ensure that the neural network outputs actual probabilities, and not some meaningless numbers. </p> <p>Diffusion models sidestep this issue by not trying to model \\(p_\\text{data}\\) directly, but an intermediate quantity which can be used to recover \\(p_\\text{data}\\) at a reasonable accuracy once learned.</p>"},{"location":"Models/00_models/#what-is-a-loss-function","title":"What is a Loss Function?","text":"<p>At a high level, people always say they \"train a model to learn something.\" Well... how does one quantify how much the model has \"learned\"?</p> <p>The loss (function) \\(L\\), also called the cost or objective, is a function people design to measure how well the model performs on a task. Conventionally, a lower loss means the model is performing better. Training a model could also be referred to as minimizing the loss.</p> Illustrative Example - House Price Prediction <p>Let's say you're trying to predict house prices. A simple and common loss function for this task could be \\(L=|\\text{TruePrice} - \\text{PredictedPrice}|^2\\), the square of the difference between the true price and the predicted price. As you can imagine, trying to minimize the loss \\(L\\) is the same as trying to reduce the difference between the true price and the prediction, or in other words make the prediction more accurate. </p> Why Minimize the Square? <p>You may ask, why minimize the difference squared and not just the difference? An intuitive explanation is this: If the difference between the true price and the predicted price is big, then the square will extrapolate it to be bigger. This means we punish the model way harder if it makes a wildly inaccurate prediction.</p> <p>There are more math-heavy reasons rooted in statistics, the details of which are out of the scope of this article. (For those interested in searchable keywords, minimizing the squared difference - the L2 loss - corresponds to maximizing the likelihood, under the assumption that the random errors are normally distributed.)</p>"},{"location":"Models/00_models/#what-is-adversarial-loss","title":"What is Adversarial Loss?","text":"<p>Adversarial loss generally refers to when practitioners pit two neural networks against each other.</p> <p>For example, in image genereating GANs, two models are simutaneously trained at once - the generator \\(G\\), and the discriminator / adversary \\(A\\):</p> <ul> <li>\\(G\\) tries its best to create realistic images and fool \\(A\\).</li> <li>\\(A\\) tries its best to distinguish between real and generated images. </li> </ul> <p>This is a sound approach, and GANs have been SOTA in terms of generative modeling. It comes with its own problem though, most prominently that it's very hard to balance \\(G\\) and \\(A\\). For example:</p> <ul> <li>\\(G\\) only learns how to exploit \\(A\\)'s defects, creating \"images\" that trick \\(A\\) but are completely unnatural.</li> <li>\\(G\\) only learns a few types of images that \\(A\\) is less certain about, destroying output variety. (Mode Collapse)</li> <li>\\(A\\) is too good at discerning real vs. generated that makes it impossible for \\(G\\) to learn from gradient descent.</li> <li>\\(G\\) and \\(A\\) end up in an infinite XYZ cycle. \\(G\\) learns to generate only X, so \\(A\\) learns to classify that as generated; \\(G\\) then learns to only generate Y, so \\(A\\) classifies all Y as generated, repeat.</li> </ul> <p>Several training rules, different types of losses, regularization techniques... have been proposed just to attempt to solve this problem in GANs.</p>"},{"location":"Models/00_models/#deep-dives","title":"Deep Dives","text":""},{"location":"Models/00_models/#p_textdata-model-specification","title":"\\(p_\\text{data}\\) Model Specification","text":"<p>A generative model is trained to take a data point from an initial probability distribution \\(p_\\text{init},\\) and output a data point from a target probability distribution \\(p_\\text{data}.\\)</p> <p>In sections above, the Gaussian was \\(p_\\text{init},\\) while the distrubtion underlying realistic images was \\(p_\\text{data}.\\) However, the entire framework is more general: it is theoretically possible to e.g. train a model that takes cats from \\(p_\\text{Cat}\\) and transform them into dogs from \\(p_\\text{Dog}.\\)</p> <p>Diffusion models learn a transformation that would warp the entire \\(p_\\text{init}\\) into the shape of \\(p_\\text{data}\\) as it steps through time \\(t.\\) Specifically, they learn velocity fields: given a mid-transform \\(p_t,\\) diffusion models learn in which directions and by how much \\(p_t\\) should change in order to get closer to \\(p_\\text{data}.\\) </p> <p>Stepping through \\(t=0\\to1\\) and taking \"snapshots\" of what the current mid-transform distrubution \\(p_t\\) looks like creates a collection of \\(p_t\\) that together trace a path - a probability path - which interpolates between \\(p_\\text{init}=p_0\\) and \\(p_\\text{data}=p_1.\\)</p> <p>During training, practitioners would pre-define a probability path during training, such as \\(x_t = (1-t)x_0 + tx_1,\\) where \\(x_0,x_t,x_1\\) are samples from \\(p_0,p_t,p_1\\) respectively. This means that during training, practitioners don't need to actually simulate the process, which would be very costly, and can calculate \\(x_t\\) directly.</p> <p>Differences in Notation</p> <p>Earlier works derived diffusion from another framework (Markov Chain Monte Carlo, MCMC), and you thus may see conflicting conventions.</p> <p>For example, some refer to the transformation \\(p_\\text{init}\\to p_\\text{data}\\) as the reverse process, preferring a time reversal convention where instead, this process steps backwards through discrete time: \\(t=1000,999,998,...,0.\\)</p> <p>Some conventions from then still linger to this day, such as data prediction being written as \\(x_0\\)-prediction and not \\(x_1\\)-prediction.</p>"},{"location":"Models/00_models/#p_textdata-model-challenges","title":"\\(p_\\text{data}\\) Model Challenges","text":"<p>Since the model should output probabilities, there are some contraints that it should follow:</p> <ol> <li>Outputs should be positive. Negative probability doesn't make sense.</li> <li>All possible outputs should sum to 1. This is because the output should represent chances that a specific outcome occurs; However, the chance that there be one outcome (any outcome, don't care what it is) should be 100%, or 1.</li> </ol> <p>1. is not hard. For example, one can simply do \\(e^{-\\text{output}},\\) where \\(\\text{output}\\) is the raw neural network output, to transform it into all strictly positive values.</p> <p>2. is the main issue. One natural idea is to first find what these outputs sum to, \\(Z,\\) then just divide the model output value by \\(Z.\\) Combining with the above, that results in \\(\\frac{e^{-\\text{output}}}{Z}.\\) </p> <p>However, as there are infinitely many possible inputs to the model, there are infinitely many possible outputs. It's impossible to sum over infinitely many arbitrary numbers in a finite amount of time.</p> <p>Here, \\(Z\\) is called the normalizing constant, as in it is the number that would normalize the sum of outputs to 1 when divided.</p> <p>People have come up with ways around this of course. Approaches prior to diffusion can be split into 3 categories, each with their own downsides. Referencing this talk / blog by Yang Song, one of the pioneers of modern diffusion, these are:</p> <ol> <li>Approximate \\(Z\\): This is still very hard and expensive to calculate.</li> <li>Restrict the Architecture: This limits the freedom in designing the neural network. Examples include autoregressive models.</li> <li>Model the Generation Process Only: Skip trying to model \\(p_\\text{data}\\) accurately, just model a process that can generate new data. This usually involves using adversarial loss, which is highly unstable and hard to train with, plus the result might deviate from \\(p_\\text{data}\\). Examples include GANs.</li> </ol>"},{"location":"Models/01_diffusion/","title":"Diffusion Models","text":"<p>Sources</p> <p>This guide mostly references MIT's lecture notes, and thus might differ from how other literature formuate things.</p> <p>For example, since flow could be thought of as a special case for diffusion, I'll be referring to both as \"diffusion\" here.</p>"},{"location":"Models/01_diffusion/#motivation-for-diffusion","title":"Motivation for Diffusion","text":"<p>Recall from #the issue in modeling \\(p_\\text{data}\\) that directly modeling \\(p_\\text{data}\\) proves problematic.</p> <p>Diffusion models instead tries to learn how to nudge Gaussian noise \\(p_\\text{init}\\) and gradually transform it into \\(p_\\text{data}.\\) </p> <p>This seemingly simple change of perspective lifts it of many drawbacks and restrictions other solutions had, leading to them becoming SOTA in fields like image and video generation, and enabling tasks like inpainting to be done.</p>"},{"location":"Models/01_diffusion/#how-2-diffuse","title":"How 2 Diffuse","text":"<p>Start by defining paths that link between the noise distribution \\(p_\\text{data}\\) and the images distribution \\(p_\\text{data}.\\) To be concrete, define the timestep \\(t\\):</p> <ul> <li>\\(t=0\\) is the start of the path, where \\(x_0\\) is pure Gaussian noise from \\(p_\\text{init}\\).</li> <li>\\(t=1\\) is the destination, where \\(x_1\\) is a realistic image from \\(p_\\text{data}\\).</li> <li>Any other \\(t\\) is somewhere inbetween, where \\(x_t\\) is some combination of data and noise.</li> </ul> <p>Following such a path is the same as gradually turning noise into clean images.</p> <p>A diffusion model learns something called a velocity field which can navigate these paths. Given a noisy image \\(x_t\\) at a specific time \\(t,\\) the velocity field shows in what way to change the pixels, such that by the time it hits \\(t=1,\\) \\(x_t=x_1\\) becomes a realistic image.</p> Illustrative Analogy <p>You just landed at the airport (\\(x_0,\\) noise) and you want to get to a restaruant (\\(x_1,\\) image), but you don't have a map. You can navigate by asking people directions:</p> <ol> <li>Ask for a direction (Query the Model).</li> <li>Walk in that direction for a small distance (Step).</li> <li>Ask again to see if the direction has changed (Repeat).</li> </ol> <p>There is a tradeoff: Asking for directions takes time. Ask every single step, and you will arrive exactly at the restaurant, but it will take all day. If you ask once and walk a mile, you might overshoot or turn down the wrong alley. In generative modeling, \"asking\" is a Neural Function Evaluation (NFE), or running the model once. More NFE leads to higher accuracy but slower generation.</p> <p>Mathematically, an equation that relates to an object and how it should change is called a Differential Equation (DE).</p> <p>Differential equations come in 2 kinds: ordinary (ODE) or stochastic (SDE). The difference is that SDEs additionally consider random noise into the equation, while ODEs don't. Ever noticed that samplers such as <code>euler_ancestral</code> never settling on a final image, instead generating wildly different compositions as one increases sampling steps? That's because it's a stochastic sampler, injecting noise at every step.</p> <p>Depending on which type of DE is chosen to model the noise-image transformation, it leads to 2 different losses:</p> <ul> <li>ODE: flow matching </li> <li>SDE: score matching</li> </ul> <p>Practitioners of today mostly use flow matching - in particular, a specific version of it called Rectified Flow (RF) - due to its simplicity and efficiency. In fact, RF is so popular that some use RF and Flow Matching interexchangeably.</p> <p>To generate an image then is to solve one of these learning ODEs or SDEs. This also means that all samplers can be thought of as ODE or SDE solvers, and ODE and SDE solvers developed before diffusion, like the Runge-Kutta methods, can be used as samplers.</p> <p>On ODE / SDE Sampling of Diffusion / Flow</p> <p>While denoising diffusion and flow matching were formulated using SDEs and ODEs respectively, it has been shown that one can construct an equivalent ODE / SDE from the other, like in Song et. al. for example.</p> <p>This means that when implemented correctly, you can use ODE methods with denoising diffusion and SDE methods with flow. </p> Differences in Notation <p>Earlier works derived diffusion from another framework (Markov Chain Monte Carlo, MCMC), and you thus may see conflicting conventions.</p> <p>For example, some refer to the transformation \\(p_\\text{init}\\to p_\\text{data}\\) as the reverse process, preferring a time reversal convention where instead, this process steps backwards through discrete time: \\(t=1000,999,998,...,0.\\)</p> <p>Some conventions from then still linger to this day, such as data prediction being written as \\(x_0\\)-prediction and not \\(x_1\\)-prediction.</p>"},{"location":"Models/01_diffusion/#latents-why-and-how","title":"Latents - Why and How","text":"<p>Directly working with images is costly - each image has hundreds of thousands of pixels, each with their rgb values.</p> <p>Latents are compressed representation of images that still retain most of the information, like colors, composition, etc. Intuitively, think of converting images to their latents as jpg compression - a little detail is lost in exchange for massively reduced file size.</p> <p>Using latents significantly reduces the compute costs for both training and image generating. This is why models working with latents - Latent Diffusion Models (LDMs) - dominate the market. <code>SD</code>, <code>Flux</code>, <code>Z-Image</code>, you name it, it probably uses latents.</p> <p>How does one compress and decompress an image from / to its latent? Well, we don't know... but we can train a neural network to do it for us! For this, #Variational AutoEncoders (VAEs) are the dominant players. </p> <p>En/Decode is Lossy</p> <p>Like jpg compression, turning images into latents and back is \"lossy\" - that is, you lose a bit of information in the process. </p> <p>This often comes up when doing many inpainting iterations: you do not want to en/decode the same image many times, lest the image quality slowly but surely degrades.</p> <p>This is also why the default ComfyUI workflow for inpainting is complete trash. Instead, use the <code>Impact</code> nodepack's detailer nodes or the <code>CropAndStitch</code> nodepack, both of which paste the inpainted region back onto the original, non-degraded image.</p>"},{"location":"Models/04_vae/","title":"VAE (Variational AutoEncoder)","text":"<p>The Variational AutoEncoder (VAE) is the most common architecture used to build models that can encode and compress complex data into smaller and simpler representations called latents. These latents can then be utilized by diffusion models, making them easier to train and run.</p> <p>Note</p> <p>I'll mainly be talking about VAEs through the lens of image generation. However, note that they can be applied to other forms of data as well. </p>"},{"location":"Models/04_vae/#preliminary-the-manifold-hypothesis-and-latent-spaces","title":"Preliminary: The Manifold Hypothesis and Latent Spaces","text":"<p>Most real-world data is \"sparse.\" For example, 512x512 rgb images exist in a 786,432-dimensional space, that is, 786,432 numbers per image. However, assigning each pixel with random rgb values will, 99.99% of the time, result in meaningless static. Only a tiny, tiny subset of all possible pixel arrangements results in coherent \"dogs\" or \"landscapes\" or other recognizable objects.</p> <p>The Manifold Hypothesis</p> <p>The Manifold Hypothesis formalizes the above idea. It suggests that real-world high-dimensional data (like natural images) actually lies on a much lower-dimensional well-behaved subspace called a manifold inside that high-dimensional space.</p> <p>This implies that rather than describing natural images in pixel space by listing every rgb pixel value, it should be possible to represent them more efficiently with a compact set of underlying features.</p> <p>A latent space \\(z\\) is the coordinate system that uses said compact feature set. </p> <p>Latent spaces are usually too complex to find manually. Instead, deep neural networks are trained to learn one such space. Though these learned spaces are far from ideal, they have enough advantages to be widely adopted.</p>"},{"location":"Models/04_vae/#autoencoders","title":"AutoEncoders","text":"<p>An AutoEncoder (AE)'s primary purpose is to learn informative latent representations of the data. An AE is simply a pair of 2 functions:</p> <ul> <li><code>Encode(x)</code>: move an image <code>x</code> to latent space</li> <li><code>Decode(z)</code>: move a latent image <code>z</code> to pixel space</li> </ul> <p>How good an AE is can be measured by how well it preserves the data. Specifically:</p> <ol> <li>Take some image <code>x</code></li> <li>Encode it to latent space <code>z = Encode(x)</code></li> <li>Decode it back to pixel space <code>x' = Decode(z)</code></li> <li><code>x</code> and <code>x'</code> should be as similar as possible.</li> </ol> <p>This is called the reconstruction quality. The loss to train an AE can also be derived from this idea, aptly named the reconstruction loss.</p> <p>However, as a standard AE is trained to map 1 data point to 1 latent point (and back), the learned latent space has 2 major flaws that make it unsuitable for generative purposes:</p> <ol> <li>Discontinuity: Points near each other in latent space may represent completely different images. This means:<ul> <li>One can't generate more cats by encoding a <code>cat</code>, varying the latent a little, then decoding it back.</li> <li>If a diffusion model uses this AE, even a little error in output can result in completely different images, making the diffusion model hard to train.</li> </ul> </li> <li>Incompleteness: The latent space contains many \"gaps\" which represent latent images that <code>Decode</code> has never seen, and thus never learned to reconstruct. Trying to <code>Decode</code> this latent will result in nonsensical images. This means:<ul> <li>One can't generate more images by randomly sampling a latent and then decoding it, because it's likely that a random latent lands in one of those pesky gaps.</li> <li>Assume an AE trained on <code>cat</code> and <code>dog</code> images, but not images containing both. If a diffusion model uses this AE, it would be impossible to generate a hybrid image because the necessary latent representation likely falls into a gap.</li> </ul> </li> </ol>"},{"location":"Models/04_vae/#variational-autoencoders","title":"Variational AutoEncoders","text":"<p>A Variational AutoEncoder (VAE) addresses both issues faced by AEs in generative modeling, by mapping inputs to probability distributions rather than single points. Which type of distribution? Why, the humble Gaussian of course! They're simple enough so it's easy to generate, yet flexible enough to express the latents.</p> <p>Now, instead of producing a specific \\(z\\), <code>Encode</code> makes a mean \\(\\bm{\\mu_z}\\) and a standard deviation \\(\\bm{\\sigma_z}\\) which together parameterize a Gaussian. Rather than receiving a specific \\(z\\), <code>Decode</code> now gets a random sample of this Gaussian \\(\\mathcal N(\\bm{\\mu_z}, \\bm{\\sigma_z}).\\) This fixes both issues that stop AEs from generative modeling:</p> <ol> <li>Continuity: As <code>Decode</code> now receives a range of inputs around \\(\\bm{\\mu_z}\\) that should all decode back into the original, it learns to map entire neighborhoods of the latent space to similar images.  </li> <li>Completeness: During training, a regularization term (KL Divergence) is introduced to \"pull\" the latent Gaussians to cluster near the origin. This creates a densely packed region without gaps, where any randomly sampled latent is guaranteed to decode into a valid image.</li> </ol> <p>However, this robustness comes at a cost: blur. The VAE tends to smooth out fine details, resulting in lossy, fuzzy images. This also shows in reconstruction, where a VAE is worse at it than AEs, tending to make the reconstructed images blurrier.</p> <p>While a VAE isn't ideal for generative modeling by itself, combining it with diffusion models proves to be a great idea: </p> <ul> <li>The VAE handles the compression, learning an informative and computationally efficient latent space.</li> <li>The diffusion model handles the generation, learning all the complex patterns and finally hone in on high-fidelity images.</li> </ul>"},{"location":"Models/04_vae/#practical-details","title":"Practical Details","text":"<p>Modern VAEs usually compress 8x8 = 64 normal pixels into 1 latent pixel. This is also why you can only specify image widths and heights in multiples of 8 - the VAE decode can only produce images whose sizes are multiples of 8.</p> <p>Each normal pixel has 3 channels: red, green, and blue. Each latent pixel has a different number of channels depending on the model. Having more channels per latent pixel means more information could be retained, but is more intense on hardware and sometimes harder to train a diffusion model with.</p> <p>Originally, most decided to go with a 4-channel VAE, including <code>SD 1.X</code> and <code>SDXL</code>. In recent times, there has been a move towards higher channel VAEs for higher quality, see page 4 here for an example on better text rendering with higher channels. </p> <ul> <li><code>SD1.X</code>, <code>SDXL</code>, <code>Pixart</code>: 4 channel VAE.</li> <li><code>Qwen Image</code>, <code>Flux 1</code>, <code>SD 3.X</code>, <code>Lumina 2.0</code>: 16 channel VAE. </li> <li><code>Flux 2</code>: 32 channel VAE.</li> <li><code>Hunyuan Image</code>: 64 channel VAE that compresses a 16x16 patch.</li> <li><code>PixelFlow</code>, <code>Chroma Radiance</code>: ditched latent space and went back to directly genning in pixels.</li> </ul>"},{"location":"Models/04_vae/#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=HBYQvKlaE0A</li> </ul>"},{"location":"Models/12_training_objectives/","title":"[DD] Training Diffusion Models","text":"<p>Recap: #How Diffusion Generates Images</p> <p>A diffusion model learns things called velocity fields that traverse paths which start at noise and end at image. This is the same as transforming noise into images.</p> <p>Mathematically, these are described by differential equations (DE), coming in two types: ordinary (ODE) and stochastic (SDE). To follow a path is to solve the corresponding DE.</p>"},{"location":"Models/12_training_objectives/#flow-and-score-matching-loss","title":"Flow and Score Matching Loss","text":"<p>Depending on whether to model the transformation from noise to image as an ODE or SDE, 2 objectives can be constructed: flow matching and score matching. A diffusion model takes 2 inputs, a noisy image \\(x_t\\) and the timestep \\(t,\\) and tries to minimize the chosen loss.</p> Aspect Flow Matching Score Matching Type ODE SDE Loss \\(\\|u_\\text{predicted}(x,t)-u_\\text{true}(x,t)\\|^2\\) \\(\\|s_\\text{predicted}(x,t)-s_\\text{true}(x,t)\\|^2\\) Predicted Quantity \\(u\\) is the velocity. Given the position (\\(x\\)) and how far along the path it is (\\(t\\)), \\(u\\) is how fast and in which direction to walk to eventually get to the path's end (a clean image). \\(s\\) is the score, defined as the gradient of the log of the probability distribution \\(s(x,t)=\\nabla_x\\log p(x,t).\\) \\(s\\) points in the direction where the image is most likely to exist. Models Most modern models use this, like <code>SD 3.X</code>, <code>Flux</code>, <code>Lumina</code>, <code>Qwen-Image</code>, <code>Wan</code>, etc. The original SD releases are noise predictors, like <code>SD 1.X</code>, <code>SDXL</code>, etc. <p>In practice, because the transformation to be modeled is from (gaussian) noise to clean image, this has many positive implications. Important ones include:</p> <ol> <li>\\(u\\) and \\(s\\) are mathematically equivalent and can be converted from one to another.</li> <li>For flow matching: This can simplify the training target down to \\(u_\\text{true}(x,t)=x_1-\\epsilon,\\) or in other words, the difference between a sample of pure noise \\(\\epsilon\\) and a completely clean image \\(x_1.\\) This is very easy and stable to train on.</li> <li>For score matching: Since one can calculate \\(u\\) from \\(s\\), the model only needs to learn the score \\(s;\\) Whereas in the most general case, both are needed for simulating SDE. Additionally, a score matching model is equivalent to a noise prediction model which predicts the noise to remove from images. Thus, these models are also called denoising diffusion models.</li> <li>While flow matching/score matching models are trained to learn ODE/SDEs respectively, due to their equivalence, both ODE and SDE samplers can be used on the either of them.</li> </ol> Oversimplifications, Differing Notation, and More Details <p>The above section is oversimplified for clarity, and uses a specific notation that may be different to other literature.</p> <ol> <li>\\(x_1-\\epsilon\\) is not the flow matching target, but a target out of many possible choices. It is however the most common, and the one based on Optimal Transport.</li> <li>Some works use a reversed time notation, where the \"start\" is the clean image and the \"end\" is the pure noise. Early works deriving diffusion from Markov Chains also may use timesteps \\(t\\) from \\(T=1000\\to0\\) rather than \\(0\\to1.\\) In any case, the general idea of learning paths whose 2 ends are data and noise, and walking this path to transform between the 2 stay the same.</li> </ol>"},{"location":"Models/12_training_objectives/#hurdles-in-score-matching","title":"Hurdles in Score Matching","text":""},{"location":"Models/12_training_objectives/#score-matching-to-noise-prediction","title":"Score Matching to Noise Prediction","text":"<p>When an image is close to being clean, score matching loss becomes numerically unstable and training breaks. Remember that I'm assuming practical conditions, then the score matching loss becomes the following (simplified):</p> \\[ L=\\underset{\\substack{\\downarrow \\\\ \\text{Near 0 when} \\\\ \\text{image is} \\\\ \\text{almost clean}}}{\\color{red}\\frac1{\\beta_t^2}}|\\beta_ts_\\text{predicted}(x,t)+\\epsilon|^2\\Rightarrow\\text{divide by 0 error} \\] <p>Thus, a \"score matching\" model is very often reparameterized and trained on a different but still mathematically equivalent objective. </p> <p>DDPM drops the red part of the original loss, and reparameterizes the score matching model into a noise prediction model (\\(\\epsilon\\)-pred, eps-pred). eps-pred saw widespread adoption afterwards.</p> \\[L=|\\epsilon_\\text{predicted}(x,t)-\\epsilon|^2\\]"},{"location":"Models/12_training_objectives/#noise-prediction-to-velocity-prediction","title":"Noise Prediction to Velocity Prediction","text":"<p>eps-pred becomes a problem again in few-steps sampling. At the extreme of 1 step, it's trying to generate a clean image from pure noise, however the eps-pred model only predicts the noise to remove from an image. Removing noise from pure noise results in... nothing. Empty. Oops, that's bad.</p> <p>That's the problem researchers of this work faced. They propose a few reparameterizations that fix this, the most influential of which being velocity prediction (v-pred): </p> \\[L=|v_\\text{predicted}(x,t)-v|^2,\\quad v=\\alpha_t\\epsilon-\\sigma_tx_1\\] <p>For v-pred, \\(\\alpha_t,\\sigma_t\\) are set in such a way which represents that the v-pred model should focus on trying to make an image, and at low noise levels it should focus on removing the remaining noise.</p> Tangential Velocity \\(v\\) and Flow Matching Velocity \\(u\\) <p>You might remember that there was also a \"velocity\" \\(u,\\) that being what the flow matching models predict. On the other hand, v-pred is also often also called velocity prediction. How do they relate to each other?</p> <p>While they come from different mathematical formulations, they coincidentally ended up with very similar results. The equation of both take the form of \\(v=\\alpha_t\\epsilon-\\sigma_tx_1.\\) However: </p> <ul> <li>\\(v\\) is interpreted as tangential velocity on a circle, where you can find a visual here. This lead them to set \\(\\alpha_t,\\sigma_t\\) to trigonometric values dependent on \\(t\\).</li> <li>\\(u\\) is straight velocity from noise directly to data. This lead them to set a constant \\(u=x_1-\\epsilon\\) (for rectified flow anyway, which is what most use)</li> </ul>"},{"location":"Models/12_training_objectives/#current-schedules-are-flawed","title":"\"Current Schedules are Flawed\"","text":"<p>Most training noise schedules at the time failed to ensure \\(x_0\\) was truly Gaussian noise, leaving behind some residual data. This oversight caused models to learn unintended correlations, such as the one between the average brightness between \\(x_0\\) and that of the final clean image \\(x_1.\\)</p> <p>During sampling, since the process always start from true Gaussian noise - which has a neutral average brightness - the model consistently generates images which lack dynamic range (the \"uniform lighting curse,\" etc).</p> <code>sd(xl)</code> And eps-gray <p>The schedule <code>sd 1.x</code> and <code>sdxl</code> used were especially horrible, essentially leaving ~7% of the original image in what should've been pure noise. Comparatively, if they chose a simple linear schedule, it would've been ~0.6%. If they chose a cosine schedule, it would've been ~0.005%. As these were the only popular models for a long time, lots of people began associating eps-pred with uniform lighting. While indeed eps can never reach true black/white, I'd argue the disastrous schedule <code>sd(xl)</code> decided to use played a bigger role.</p> <p>The fix was straightforward: Ensure \\(x_0\\) is actually pure noise. Or in technical jargon, use a Zero Terminal Signal-to-Noise Ratio (ZTSNR) schedule. </p> <p>The authors also recommend using v-prediction instead of epsilon-prediction, since the latter can't learn from pure-noise inputs.</p> <p>They also find that a \"trailing\" schedule is more efficent than others. In common UIs, that means switching from the <code>normal</code> schedule to the <code>sgm_uniform</code> schedule.</p> Why is it called ZTSNR? <p>It's a mouthful, but it isn't that difficult to understand once you break the words apart.</p> <ul> <li>\"Terminal\" simply means \"final.\" In this case, as the authors are using the time-reversal framework, this is when it should be pure noise.</li> <li>\"Signal-to-Noise Ratio\" (SNR) is the ratio between the information and the noise squared, \\((\\frac{\\text{signal}}{\\text{noise}})^2.\\) Pure noise is desired, so that means there's 0 signal. \\((\\frac0{\\text{noise}})^2=0.\\)</li> </ul> <p>So \"Zero Terminal Signal-to-Noise Ratio\" simply means at the last (terminal) timestep, there is no signal and only noise (SNR = 0).</p>"},{"location":"Models/99_notes/","title":"Additional Notes","text":"<p>UNDER CONSTRUCTION</p>"},{"location":"Models/99_notes/#in-relation-with-literacy","title":"In Relation with Literacy","text":""},{"location":"Sampling/00_sampling/","title":"What is Sampling?","text":"<p>Sampling has already been explained in other sections in this guide. To recap:</p> <ul> <li>#What: sampling means to pick data out according to a data distribution \\(p_\\text{data},\\) which assigns probabilities to data. For example, a cat distribution \\(p_\\text{Cat}\\) would assign high probability to cats, and low probability to everything else like dogs, humans, scenery, ...</li> <li>#How: For diffusion models, solve the corresponding ODE/SDE which describe paths that take Gaussian noise and gradually transform it into realistic images.</li> </ul>"},{"location":"Sampling/00_sampling/#what-is-a-sampler","title":"What is a Sampler?","text":"<p>While not all of them were conceived this way, all diffusion samplers can be viewed as SDE/ODE solvers - or in other words, numerical integration methods.</p>"},{"location":"Sampling/00_sampling/#deep-dive-sampling-algorithm","title":"[Deep Dive] Sampling Algorithm","text":"<p>Using the Euler method, one can sample from a diffusion model as follows:</p> <p>Algorithm 1: Diffusion Sampling (Euler-Maruyama Method)</p> <p>Require: diffusion model \\(u_\\theta,\\) step sizes \\([h_0,h_1,...,h_{n-1}]\\), diffusion coefficient \\(\\eta_t\\)</p> <ol> <li>Set \\(t = 0\\)</li> <li>Draw a sample \\(x_0\\sim p_{\\text{init}}\\)</li> <li>for \\(i=0,...,n-1\\) do</li> <li>\u2003Draw random noise \\(\\epsilon \\sim \\mathcal N(0,1)\\)</li> <li> \\(x_{t+h}=x_t+h_iu_\\theta(x_t,t)+\\eta_t\\sqrt{h_i}\\epsilon\\)</li> <li>\u2003Update \\(t\\gets t+h_i\\)</li> <li>end for (\\(t\\) should be \\(1\\) after the loop)</li> <li>return \\(x_1\\)</li> </ol> <p>Where \\(\\mathcal N(0,1)\\) is the Standard Gaussian distribution. </p> <p>A few notes:</p> <ul> <li>We recover flow matching / ODE sampling by setting \\(\\eta_t=0.\\)</li> <li>One can adapt the above algorithm to other samplers by changing line 5 to using said samplers' update rules rather than Euler's.</li> </ul> <p>To adapt the above into <code>k-diffusion</code>, and by extension popular UIs like forge and comfy, a few terminologies need to shift, as it mostly follows Karras's convention. Mainly:</p> <ol> <li><code>k-diffusion</code> works with a denoiser model \\(D_\\theta\\) that would predict the clean image directly, rather than \\(u_\\theta\\) which predicts the change needed to get there. </li> <li><code>k-diffusion</code> directly works in a noise schedule \\(\\sigma_t\\) rather than step sizes.</li> </ol> <p>These aren't huge issues, as one can be translated into another without much trouble.</p> <p>Algorithm 2 mostly follows the conventions in Algorithm 1, and thus you may find it easier to follow the changes; Algorithm 2.1 is a more direct inscription of the actual code.</p> <p>Algorithm 2: <code>k-diffusion</code> Sampling (Euler-Maruyama Method)</p> <p>Require: data prediction model \\(D_\\theta,\\) noise schedule \\([\\sigma_0,\\sigma_1,...,\\sigma_n]\\), diffusion coefficient \\(\\eta\\)</p> <ol> <li>Draw a sample \\(x\\sim\\mathcal N(0, \\sigma_0^2)\\)</li> <li>for \\(i=0,...,n-1\\) do</li> <li>\u2003Draw random noise \\(\\epsilon\\sim\\mathcal N(0,1)\\)</li> <li>\u2003Set \\(h_\\text{down},h_\\text{up}\\gets\\text{find\\_step\\_size}(\\sigma_i,\\sigma_{i+1},\\eta)\\) </li> <li>\u2003Set \\(u_\\theta\\gets (x-D_\\theta(x,\\sigma_i))/\\sigma_i\\)</li> <li>\u2003Update \\(x\\gets x+h_\\text{down}u_\\theta+h_\\text{up}\\epsilon\\)</li> <li>end for </li> <li>return \\(x\\)</li> </ol> <p>Algorithm 2.1: <code>k-diffusion</code> Sampling Truer-to-the-Code (Euler-Maruyama Method)</p> <p>Require: denoiser model \\(D_\\theta,\\) noise schedule \\([\\sigma_0,\\sigma_1,...,\\sigma_n]\\), diffusion coefficient \\(\\eta\\)</p> <ol> <li>Draw a sample \\(x\\sim\\mathcal N(0, \\sigma_0^2)\\)</li> <li>for \\(i=0,...,n-1\\) do</li> <li>\u2003Draw random noise \\(\\epsilon\\sim\\mathcal N(0,1)\\)</li> <li>\u2003Set \\(\\sigma_\\text{down},\\sigma_\\text{up}\\gets\\text{get\\_ancestral\\_step}(\\sigma_i,\\sigma_{i+1},\\eta)\\) </li> <li>\u2003Set \\(d\\gets (x-D_\\theta(x,\\sigma_i))/\\sigma_i\\)</li> <li>\u2003Set \\(dt\\gets \\sigma_\\text{down}-\\sigma_i\\)</li> <li>\u2003Update \\(x\\gets x+d\\cdot dt+\\sigma_\\text{up}\\epsilon\\)</li> <li>end for </li> <li>return \\(x\\)</li> </ol>"},{"location":"Sampling/01_accuracy/","title":"\"Accuracy / Control\"","text":"<p>You've probably seen something like this in other diffusion guides:</p> <p><code>DPM++</code> is better [than <code>Euler</code>] for \"accuracy\" / \"precision\" / \"control\".</p> <p>Viewing sampling through the lens of solving the diffusion differential equation (DE), it becomes clearer what this could mean - to solve the DE more accurately. In math terms, we'd say that the more accurate solver is higher order.</p> <p>However, note we're simply looking to generate nice images. Numerical accuracy does not directly translate to good-looking images.</p> <p>For example, the ever-popular <code>euler(_ancestral)</code> is actually the most inaccurate sampler there is. The errors it make manifest as blurring the output and create a soft/dreamy visual style that many find pleasing. </p> <p>Inaccuracies don't always play out nicely though. For example, some potential drawbacks may include:</p> <ul> <li>Small detail is lost: backgrounds merging into meaningless blobs, hair strands losing definition, etc.</li> <li>Worse prompt adherence: In severe cases, the errors could become so big that it actively hurts how much the image follows your prompt.</li> </ul> <p>On the other hand, diffusion DEs are very stiff, especially at high CFG - this increases numerical instability and in practice:</p> <ol> <li>Makes <code>adaptive</code> solvers take tiny steps = very long to generate an image</li> <li>Makes higher order solvers unstable and do worse, completely breaking in severe cases, especially at low steps</li> </ol> <p>This may be why the community favorites - <code>euler_ancestral</code> and <code>dpmpp_2m</code> - are \"only\" first-order and second-order respectively. (And <code>dpm(pp)</code> uses a clever math trick called the exponential integrator to make it less stiff)</p> <p>Info</p> <p>Did you know that as originally formulated in DDPM, a diffusion model has to take 1000 steps to generate an image? What we're doing now all seems like \"low step count\" compared to that!</p>"},{"location":"Sampling/01_accuracy/#how-does-order-affect-error","title":"How Does Order Affect Error?","text":"<p>Oversimplification</p> <p>I'll be brushing over many details and oversimplifying things for ease of understanding here. For more accurate information on this topic, see truncation error.</p> <p>Order measures how much the errors a sampler makes scale down when you decrease the step size - or equivalently, increase the number of steps.</p> <p>Let's assume for simplicity that the error of any sampler taking 1 step is 10. As in, by some measure, the difference between the truth and the answer produced by a sampler is 10.</p> <p>Now, let's take <code>euler</code>, <code>heun</code>, and <code>bosh3</code>, which have an order of 1, 2, 3 respectively, and look at the error at various steps:</p> steps <code>euler</code> <code>heun</code> <code>bosh3</code> 2 <code>10 / 2</code> = 5 <code>10 / (2*2)</code> = 2.5 <code>10 / (2*2*2)</code> = 1.25 3 <code>10 / 3</code> \u2248 3 <code>10 / (3*3)</code> \u2248 1.1 <code>10 / (3*3*3)</code> \u2248 0.37 4 <code>10 / 4</code> = 2.5 <code>10 / (4*4)</code> = 0.625 <code>10 / (4*4*4)</code> \u2248 0.156 <code>n</code> <code>10 / n</code> <code>10 / (n*n)</code> <code>10 / (n*n*n)</code> <p>In general, if the order of a sampler is <code>O</code>, and the error it makes when you take 1 step is <code>E</code>, then the error if you take <code>N</code> steps would be around <code>E / (N^O)</code>.</p> <p>Let's compare <code>euler</code> and <code>heun</code>. <code>heun</code> takes twice as long as <code>euler</code> per step, and has an order of 2. Now, let's run them for the same amount of time and see what happens to the error:</p> <ul> <li><code>euler</code> for 10 steps, the error is about <code>10 / 10</code> = 1</li> <li><code>heun</code> for 5 steps (because it takes 2x as long), the error is about <code>10 / (5*5)</code> = 0.4</li> </ul> <p>So in theory, you can get better bang (accuracy) for your buck (time) by using higher order samplers. </p> <p>The high stiffness of diffusion DEs makes out-of-the-box high-order samplers do poorly though. Using stiff-resistant techniques is recommended, for example:</p> <ul> <li>Implicit methods like <code>gauss-legendre</code>. They're not really used in practice, in favor of...</li> <li>Exponential integrators, beginning with <code>deis</code>, and most ODE samplers that came after, including the <code>dpm(pp)</code> family, <code>res</code> (Refined Exponential Solver), etc.<ul> <li>You may see some unfamiliar names here, like <code>cox-matthews</code>, <code>strehmel-weiner</code>, <code>hochbruck-ostermann</code>, <code>krogstad</code>, ... To clarify, these are people who developed the coefficients for exponential integrator solvers. To see more specifics about them, check the #exponential integrators section.</li> </ul> </li> </ul>"},{"location":"Sampling/03_sample_speed/","title":"Generation Speed and How to Go Faster","text":"<p>Running the neural network is the most computationally expensive operation by far, when it comes to generating images with diffusion. The time taken for a generation is mostly determined by how many times the neural network needs to be ran. Running the neural network is also called \"calling/evaluating the model,\" doing a \"model call/evaluation,\" etc.</p> <p>This is why in a sampler speed comparison chart, one can notice that most of they seem to run in integer multiples of <code>euler</code> speed, e.g. <code>heun</code> is about 2x slower than <code>euler</code>: <code>euler</code> runs the model once per step, making it a great unit to compare against; <code>heun</code> runs the model twice per step, hence it's 2x slower.</p> <p>This is also why researchers don't compare samplers in number of steps, but Number of Function Evaluations (NFE), or in other words how many times the model was run. It wouldn't make practical sense to compare say <code>euler</code> and <code>heun</code> both at 20 steps, because the latter would have twice the NFE and run for twice as long.</p> <p>Steps is Not Time Spent</p> <p>Be on the lookout for potentially misleading statements, such as: \"<code>dpmpp_sde</code> is great for low steps sampling.\" While technically true, <code>dpmpp_sde</code> runs the model twice per step, which means that in the time <code>dpmpp_sde</code> runs for 5 steps, you could've run say <code>dpmpp_2m</code> for 10 steps. Make sure to take this into account when making speed-quality comparisons.</p>"},{"location":"Sampling/03_sample_speed/#some-things-affecting-sampling-speed","title":"Some Things Affecting Sampling Speed","text":"<p>Certain samplers run the model multiple times per step to achieve higher accuracy. See the #sampler tl;dr for details.</p> <p>Increasing the image resolution means the model has to ingest and output more information, making it take longer.</p> <p>In each step of generating an image, the model has to predict how to change the current intermediate output to move it closer to an image described by the positive prompt.</p> <ul> <li>Enabling negative prompts by setting #cfg to anything but 1 doubles the amount of work and thus halves the speed. The model now additionally needs to predict on the negative prompt (to move away from it).</li> <li>Perp-Neg makes negative prompts very strong, but the model now also needs to predict on the empty prompt. This means 1.5x slower than cfg-not-1 and negative prompts are in play, or 3x slower than if cfg is 1.</li> </ul>"},{"location":"Sampling/03_sample_speed/#some-things-not-affecting-sampling-speed","title":"Some Things NOT Affecting Sampling Speed","text":"<p>Schedulers only determine the noise levels of each step, the number of model runs should be unaffected.</p> <p><code>ancestral</code> or <code>sde</code> variants need to spend a bit of time manipulating the noise each step, but the time used for this is tiny compared to calling the model. They largely run as fast as their original version.</p> <p><code>cfg_pp</code> variants redo how cfg works, but doesn't change the number of model calls.</p>"},{"location":"Sampling/03_sample_speed/#table-of-sampler-speeds","title":"Table of Sampler Speeds","text":"<p>For convenience, below is a table of commonly found samplers and how much longer they take to complete 1 iteration, relative to <code>euler</code>. 1x means about the same speed, 2x means twice as slow, etc. </p> How much slower Sampler 1x <code>euler, euler_cfg_pp, euler_ancestral, euler_ancestral_cfg_pp, dpm_fast, dpmpp_2m, dpmpp_2m_cfg_pp, dpmpp_2m_sde, dpmpp_2m_sde_gpu, dpmpp_3m_sde, dpmpp_3m_sde_gpu, ddpm, lcm, ipndm, ipndm_v, deis, res_multistep, res_multistep_cfg_pp, res_multistep_ancestral, res_multistep_ancestral_cfg_pp, gradient_estimation, gradient_estimation_cfg_pp, er_sde, sa_solver, ddim, uni_pc, uni_pc_bh2</code> 2x <code>heun, dpm_2, dpm_2_ancestral, dpmpp_2s_ancestral, dpmpp_2s_ancestral_cfg_pp, dpmpp_sde, dpmpp_sde_gpu, seeds_2, sa_solver_pece, exp_heun_2_x0, exp_heun_2_x0_sde</code> 3x <code>heunpp, seeds_3</code>"},{"location":"Sampling/04_types/","title":"Types of Sampling Methods","text":""},{"location":"Sampling/04_types/#adaptive","title":"<code>adaptive</code>","text":"<p>Samplers that choose their own steps, ignoring your setting for step count and scheduler. In some implementations, <code>steps</code> may instead be used as the \"max steps\" before it's forcefully stopped lest it takes too long.</p>"},{"location":"Sampling/04_types/#stochastic-sde-ancestral-a-sa_solver-restart","title":"Stochastic (<code>SDE</code>, <code>ancestral (a)</code>, <code>sa_solver</code>, <code>Restart</code>)","text":"<p>Samplers that inject noise back into the image. They never converge - with higher and higher step counts, they don't land on 1 final image and keep refining, instead, the composition may drastically change even if it's very late down the line.</p> <p>The theoretical quality of images generated based on non-stochastic vs. stochastic sampling depends on step count:</p> <ul> <li>low steps: samplers make a few big errors (low steps, high step size). Non-stochastic samplers usually make errors smaller than stochastic samplers if you compare 1 step of each. Thus, non-stochastic methods do better than stochastic methods in low steps.</li> <li>high steps: samplers make many small errors (high steps, small step size), which build up over time. It's now the accumulated error affecting image quality the most, and the random noise introduced by stochastic methods can gradually correct them. Thus, stochastic methods do better than non-stochastic methods in high step counts.</li> </ul> <p>Stochastic Methods In Low Steps</p> <p>In practice, it's almost always better to use stochastic samplers if you don't care about non-convergence.</p> <p>Many new stochastic methods also try to incorporate the best of both worlds, working nicely even in low steps. This includes <code>Restart</code>, <code>er_sde</code>, <code>sa_solver</code>, and <code>seeds</code>.</p>"},{"location":"Sampling/04_types/#why-stochasticity-break-flux-and-more","title":"Why Stochasticity Break Flux and More","text":"<p>SD 3.X, Flux, AuraFlow, Lumina 2, and potentially more to come, were all trained on a schedule which is very sensitive to the variance (a statistical measure) of the data. </p> <p>Without careful calibration, chances are that your stochastic sampler makes the variance increase without bound, thus breaking these models. This is why people weren't having luck using anything <code>ancestral</code> or <code>sde</code> etc. on them.</p>"},{"location":"Sampling/04_types/#singlestep-s-multistep-m","title":"<code>singlestep (s)</code> / <code>multistep (m)</code>","text":"Feature Singlestep (s) Multistep (m) How it works Runs the model multiple times each step for better accuracy Considers multiple previous steps for better accuracy Model Calls per Step <code>k</code> 1 Speed (per step) <code>k</code> times slower than 1 <code>euler</code> Basically as fast as <code>euler</code> Accuracy High Lower than <code>singlestep</code> Example <code>dpmpp_2s_ancestral</code> (2 model calls per step = 2x slower than <code>euler</code>) <code>dpmpp_2m</code> (same speed as <code>euler</code>)"},{"location":"Sampling/04_types/#implicit-explicit","title":"Implicit / Explicit","text":"<p>2 approaches used to solve DEs. </p> <p>Implicit methods solve a harder form of the DE, making them slower but more resistant to stiffness. This means in theory, you can use higher order implicit methods without them breaking, leading to moar accuracy. (This is super slow, though.)</p> <p>ALL common samplers are explicit. This includes <code>euler</code>, <code>deis</code>, <code>ipndm(_v)</code>, <code>dpm(pp)</code> family, <code>uni_pc</code>, <code>res_multistep</code>, and more.</p> <p>Implicit methods are usually not used in favor of those based on #Exponential Integrators. The quality-speed tradeoff of implicit methods seem to limit their popularity. They're also not found as defaults in popular UIs.</p> <p>Where Can I Find Implicit Samplers?</p> <p>ComfyUI: RES4LYF</p>"},{"location":"Sampling/04_types/#diagonally-implicit","title":"Diagonally Implicit","text":"<p>These are a subset of implicit methods whose difficulty to solve sits between implicit and explicit methods. They're easier to solve, but can't get as accurate.</p> <p>Here's a comprehensive review of Diagonally Implicit Runge Kutta (DIRK) methods for the interested.</p>"},{"location":"Sampling/04_types/#training-free-training-based","title":"Training-Free / Training-Based","text":"<p>Training-free methods are those that you can use without further changing the model. You can simply load the model in and use a training-free sampling method on it and it'll (probably) work. These include familiar faces like <code>euler</code>, <code>dpmpp_2m</code>, etc.</p> <p>Training-based methods require further modification of the model. This would include LCM, Lightning, Hyper, etc. where in order to use them you need to download a separate version of a model or use a LoRA. The upside is generating images in vastly lower steps like 8 or 4.</p> <p>Though sometimes they come along with a dedicated sampler like <code>lcm</code>, they may work in tandem with training-free samplers in general. For example, you can probably use any of <code>euler</code>, <code>dpmpp_2m</code>, and more on a model with a Hyper LoRA applied.</p>"},{"location":"Sampling/04_types/#exponential-integrators","title":"Exponential Integrators","text":"<p>Exponential Integrators (EI), sometimes called Exponential Time Differencing (ETD) among its many other names, became active research at least 10-20 years before diffusion models were developed. It breaks DEs down into a stiff \"linear part,\" and the less-stiff \"non-linear part\":</p> \\[ \\frac{du}{dt} = \\underbrace{Au}_{\\text{Linear(very stiff)}} + \\underbrace{f(u, t)}_{\\text{Non-linear}} \\] <p>The stiff linear part can then be solved exactly, leaving only the non-linear part to the sampler/solver. Compared to traditional methods, exponential integrators eliminate all errors associated with approximating the stiff linear part, making them great for stiff diffusion DEs.</p> <p><code>deis</code>, <code>dpm(pp)</code>, <code>unipc</code>, <code>res</code> (and probably more) all use this technique.</p> <p>You can find a technical evaluation of some exponential integrators on physics problems here. (as well as something called \"Lawson methods,\" or \"Integrating Factor (IF) methods.\")</p>"},{"location":"Sampling/05_training_free_list/","title":"(Non-exhaustive) List of Training-Free Samplers","text":"<p>At the beginning of each section, you may find a table summarizing the samplers mentioned in the section.</p> Sampler Time Spent Order Converges Notes Name of the sampler How many times slower than <code>euler</code>. 1x=the same as <code>euler</code>, 2x=takes twice as long as <code>euler</code>, etc The order, as mentioned in #accuracy/control. Some technically support a range of orders, in that case, I'll include the default &amp; range. Yes (refines 1 image with more steps) / No (may change composition with more steps) Some notes about the sampler"},{"location":"Sampling/05_training_free_list/#explicit-runge-kutta-methods-euler-heun-and-beyond","title":"Explicit Runge-Kutta Methods: Euler, Heun, and Beyond","text":"Sampler Time Spent Order Converges Notes <code>euler</code> 1x 1 Yes Simplest and most inaccurate, makes soft lines &amp; blurs details. <code>euler_ancestral</code> 1x 1 No Like <code>euler</code> but divergent (adds noise), popular. <code>heun</code> 2x 2 Yes Can be thought of as the \"improved\" <code>euler</code> <code>bosh3</code> 3x 3 Yes 3rd order RK <code>rk4</code> 4x 4 Yes 4th order RK <code>dopri6</code> 6x 5 Yes 6 model calls/step is needed for order 5. <p><code>euler</code>, <code>heun</code>, and the rarer <code>fehlberg2</code>, <code>bosh3</code>, <code>rk4</code>, <code>dorpi6</code>, and more, all fall under the umbrella of explicit Runge-Kutta(RK) Methods for solving ODEs. They were developed way before any diffusion model, or even any modern computer, came to be.</p> <p>RK methods are singlestep, which means that the higher order ones take a while to run. <code>bosh3</code> for example takes 3 times longer than <code>euler</code> per step. Combined with the fact that diffusion DEs are stiff, this means that it's seldom worth using a high-order explicit RK method by itself, as it massively increases sampling time while netting you a very marginal gain in quality. Personally, I'd at most use <code>bosh3</code>, though even that's cutting it close. It's no wonder that you don't find some of these in popular UIs.</p> <p>Samplers developed down the road employ optimizations specific to diffusion equations, making them the better choice 90% of the time. </p> <p>Where Can I Find Higher Order RK Samplers?</p> <ul> <li>ComfyUI: ComfyUI-RK-Sampler or RES4LYF</li> <li>reForge: Settings-&gt;Sampler according to this</li> </ul>"},{"location":"Sampling/05_training_free_list/#early-days-of-diffusion-ddpm-ddim-plmspndm","title":"Early Days of Diffusion: DDPM, DDIM, PLMS(PNDM)","text":"Sampler Time Spent Order Converges Notes <code>ddpm</code> 1x 1 No The original diffusion sampler <code>ddim</code> 1x 1 Yes Converges faster than DDPM, trading a bit of quality <code>plms</code>=<code>pndm</code> 1x default 4 (up to 4) Yes LMS tailored for use in diffusion (uses Adams\u2013Bashforth under the hood) <p>DDPM was what started it all, applying diffusion models to image generation, achieving really high quality but requiring a thousand steps to generate a sample. </p> <p>Through adjustments to the diffusion equation, people arrived at DDIM, drastically reducing the number of steps required at the cost of a little quality.</p> <p>PNDM finds that classical methods don't work well with diffusion equations. So they design a new way to do it - pseudo numerical methods. They then tried many approaches and found that Pseudo Linear Multistep Method (PLMS) seemed the best, hence the other name.</p> <p>These 3, along with the classical ODE solvers <code>euler</code>, <code>heun</code>, and <code>LMS</code>, were the original samplers shipped with the release of the original stable diffusion.</p>"},{"location":"Sampling/05_training_free_list/#steady-improvements-deis-ipndm-dpm","title":"Steady Improvements: DEIS + iPNDM, DPM","text":"Sampler Time Spent Order Converges Notes <code>deis</code> 1x default 3 (up to 4) Yes <code>ipndm</code> 1x 4 Yes <code>ipndm</code> found empirically better than <code>ipndm_v</code> <code>ipndm_v</code> 1x 4 Yes <code>dpm_2</code> 2x 2 Yes <code>dpm_2_ancestral</code> 2x 2 No <code>dpm_adaptive</code> 3x default 3 (2 or 3) Yes ignores steps &amp; scheduler settings, runs until it stops itself <code>dpm_fast</code> 1x (averaged) between 1~3 Yes Uses DPM-Solver 3,2,1 such that the number of model calls = number of steps, effectively taking the same time as <code>euler</code> would in the same number of steps <p>DEIS and DPM independently came to the same conclusion: Diffusion equations are too stiff for classical high-order solvers to do well. They use a variety of techniques to remedy this.</p> <p>Notably, they both solve a part of the equation exactly, removing any error associated with it while leaving the rest less stiff. This idea is  called #exponential integrators, and it's so good that many samplers down the road also do it.</p> <p>The DEIS paper also introduced \"improved PNDM\" (iPNDM). <code>ipndm_v</code> is the variable step version that should work better for diffusion, though they find empirically that <code>ipndm</code> performs better than <code>ipndm_v</code>.</p> Differing Results With <code>ipndm_v</code> <p>In my personal tests in ComfyUI, for some reason, I find for the same exact parameters - prompt, seed, etc. - <code>ipndm_v</code> sometimes breaks (lots of artifacts) if you use <code>KSampler</code>, but not if you use <code>SamplerCustomAdvanced</code>. In fact, I've never gotten any image breakdown with <code>SamplerCustomAdvanced</code> + <code>ipndm_v</code>, unless at low steps where it hasn't converged. Bextoper has also noted that <code>ipndm_v</code> breaks similarly to <code>KSampler</code> in Forge.</p>"},{"location":"Sampling/05_training_free_list/#cascade-of-new-ideas-dpm-unipc-restart-res-gradient-estimation-er-sde-seeds","title":"Cascade of New Ideas: DPM++, UniPC, Restart, RES, Gradient Estimation, ER SDE, SEEDS","text":"Sampler Time Spent Order Converges Notes <code>dpmpp_2s_ancestral</code> 2x 2 No \"<code>dpmpp</code>\" as in \"DPM Plus Plus\" = \"DPM++\" <code>dpmpp_sde</code> 2x 2 No I think this is \"SDE-DPM-Solver++(2S)\" not found explicitly defined in the paper <code>dpmpp_2m</code> 1x 2 Yes <code>dpmpp_3m_sde</code> 1x 3 No <code>uni_pc</code> 1x 3 Yes Official repo <code>uni_pc_bh2</code> 1x 3 Yes Empirically found a little better than <code>uni_pc</code> in guided sampling <code>Restart</code> default 2x (varies) default 2 (varies) No Time Spent &amp; order depends on the underlying solver used (paper uses <code>heun</code>); Official repo <code>res_multistep</code> 1x 2 Yes The authors give a general way to define <code>res_singlestep</code> for any order <code>gradient_estimation</code> 1x 2 (?) Yes Uses 2 substeps, so I guess order 2? Not sure if the notion of order really applies... <code>seeds_2/3</code> 2/3x 2/3 No <code>er_sde</code> 1x default 3 (1-3) No Official repo <p>*(this nothing to do with StableCascade I just thought it's a cool title)</p> <p>Around this time, the idea of guidance took off, offering the ability to specify what image we want to generate, but also bringing new challenges to the table:</p> <ul> <li>High guidance makes the DE even stiffer, breaking high-order samplers</li> <li>High guidance knocks samples out of the training data range (train-test mismatch), creating unnatural images</li> </ul> <p>To address issues with high CFG, DPM++ adds 2 techniques (that were proposed in prior works by others already) to DPM:</p> <ul> <li>Switch from noise (eps, \\(\\epsilon\\)) prediction to data (\\(x_0\\)) prediction (which they show is better by a constant in Appendix B).</li> <li>The above also allows them to apply thresholding to push the sample back into training data range.</li> </ul> DPM++: Practical, Not a Technical Marvel <p>The <code>dpmpp</code> family, especially <code>dpmpp_2m</code>, are one of the most widely used samplers alongside <code>euler_ancestral</code>. However, it was rejected by ICLR due to heavily relying on existing works, so it \"does not contain enough technical novelty.\"</p> <p>UniPC came soon after. Inspired by the predictor-corrector ODE methods, they develop UniC, a corrector that can be directly plugged after any existing sampler to increase its accuracy. As a byproduct, they derive UniP from the same equation as UniC, which is a predictor that can go up to any arbitrary order. The two combine to UniPC, achieving SOTA results using order=3.</p> <p>Restart doesn't actually introduce a new sampler, instead focusing on the discrepancy between trying to solve diffusion as an ODE (no noise injections) vs. an SDE (injects noise at every step). To get the best of both worlds, Restart proposes that rather than injecting noise at every step, let's do it in infrequent intervals.  Visualization of ODE, SDE, and Restart taken from their official repo</p> <p>RES examined the order conditions that solvers must satisfy to achieve their claimed accuracy. They find for example <code>dpmpp</code> doesn't satisfy some of these, leading to worse-than-expected results. They then unify the equation for noise prediction and data prediction, making analysis easier. Finally, they pick coefficients that satisfy these additional conditions.</p> On RES's Contributions <p>It's worth noting that RES did not discover the order conditions nor the coefficients satisfying them; those achievements mainly go to Hochbruck and Ostermann's works on Exponential Integrators. What RES did do is apply these already-known theories to diffusion samplers at the time.</p> <p>Gradient Estimation finds that denoising can be interpreted as a form of gradient descent, and designs a sampler based on it.</p> <p>SEEDS rewrites a whole lot of equations so that more parts can be solved exactly or approximated more accurately. To make sure the equation stays true, a modified way of injecting noise is used. They derive SEEDS for both eps-pred and data-pred, though the former is very slow so ComfyUI includes only the latter.</p> <p>ER SDE models the diffusion process as an Extended Reverse-Time SDE and develops a solver based on that.</p>"},{"location":"Sampling/05_training_free_list/#out-of-reach-amed-dc-solver-among-others","title":"Out of Reach: AMED, DC-Solver, among Others","text":"<p>With so many samplers, it's no surprise that some have been left out of the party. This section includes sampling techniques that, while exist in literature or python code, are unavailable in popular UIs that are more accessible to non-coders. Since they're not widely available, discussion is also low so there will be many more that I simply don't know about and are missing from the list.</p> <p>Techniques Not Included: (in no particular order)</p> <ul> <li>UniC: As said before, you could in theory plug UniC after any sampler to achieve better accuracy. No UI lets you actually do that though to my knowledge.</li> </ul> <p>Samplers Not Included: (in no particular order)</p> <ul> <li>AMED</li> <li>DC-Solver</li> <li>GottaGoFast</li> <li>Era-Solver</li> <li>SciRE-Solver</li> <li>gDDIM</li> <li>Analytic-DPM</li> </ul>"},{"location":"Sampling/05_training_free_list/#tackling-stiffness-efficiently-exponential-integrators","title":"Tackling Stiffness Efficiently: Exponential Integrators","text":"Sampler Time Spent Order Converges Notes <code>deis</code>, <code>dpm(pp)</code>, <code>unipc</code>, ... Details in previous sections <code>res_&lt;k&gt;s(_name)</code> <code>k</code>x <code>&lt;= k</code> Yes See below <p>Where Can I Find More Exponential Integrator Samplers?</p> <ul> <li>ComfyUI: RES4LYF, under <code>exponential/</code></li> </ul> <p>See more details about Exponential Integrators #here. </p> <p>We can borrow the solvers that researchers in this field have developed to use as samplers, such as <code>cox-matthews</code>, <code>strehmel-weiner</code>, <code>hochbruck-ostermann</code>, <code>krogstad</code>, etc. Unfortunately, I'm unaware of any frontend that allows you to easily access these, other than using ComfyUI with the <code>RES4LYF</code> extension. </p>"},{"location":"Sampling/06_training_based_list/","title":"(Non-exhaustive) List of Training-Based Sampling Methods","text":"<p>UNDER CONSTRUCTION</p> <p>This section is incomplete and subject to massive changes. </p> <p>Applicability</p> <p>Note that this section is mostly written with SDXL in mind. Others such as Z-Image-Turbo doesn't use the same techniques as the <code>turbo</code> mentioned here.</p> <ul> <li><code>lcm</code>: Consistency Models defines a new training objective for the model - to learn the \"consistency function (CF),\" which is some function <code>f(x, t) = x_0</code> that takes the current noisy image and the time step, and outputs the origin - that is, the completely denoised image. LCM applies the idea to latent-based models (like stable diffusion).<ul> <li>Sampling in 1 step in practice doesn't yield great images. To fix this, they enable it to run multiple steps by re-injecting noise. The procedure is: 1st prediction, inject noise back, predict again, ... repeat.</li> </ul> </li> <li><code>turbo</code>: Paper authors use both adversarial loss and distillation loss to further train a model to allow it to generate images in a few steps.<ul> <li>Adversarial loss: A neural network (the \"discriminator\") is used to \"detect\" if the few-step-generated image is AI-generated; If the discriminator successfully detects it, the imagegen model is punished and forced to do better. The discriminator is also trained by us and not perfect. Only using adversarial loss eventually leads to the model to exploit the discriminator's defects that, while minimizing adversarial loss, results in horrible images.</li> <li>Distillation loss: During training, which requires the model being trained (the \"student\") to predict the noise, a different imagegen model (the \"teacher\") also predicts the noise along with it. The student is rewarded if what it predicts is close to what the teacher predicts. This helps ground the student model to generate sane images and not abuse the discriminator's imperfections.</li> </ul> </li> <li><code>lightning</code>: Basically improves upon the methods of <code>turbo</code>. <code>turbo</code> used a pre-trained encoder as the foundation for the discriminator, which introduced several problems like increased training hardware reqs, stuff like LoRAs being less compatible, etc. <code>lightning</code> directly uses the diffusion model's encoder as the discriminator's backbone instead.</li> <li><code>tcd</code>: Authors identify issues on why <code>lcm</code> can't make clear images with good detail, which is the accumulated errors due to practical limitations during training and sampling. To fix this they:<ul> <li>Update the training objective: They expand the definition of the original CF and arrive at the \"trajectory consistency function (TCF),\" <code>f(x, t, s) = x_s</code>. Compared to the original CF, TCF additionally takes an input <code>s</code>, and outputs the partially denoised image at timestep <code>s</code> (if <code>s = 0</code> then the output is the completely denoised image again).</li> <li>Update the sampling method: The new objective being TCD allows them to use stochastic sampling, which helps to correct accumulated errors.</li> <li>Several key ideas of TCD are highly similar to that of CTM and the two have a dispute. TCD has been accused of plagiarism by CTM's authors, with there being a response on the same post and TCD's code repo, with CTM's authors being dissatisfied with said response in this issue. I can't find anything that happened afterward.</li> </ul> </li> <li><code>hyper</code>: Takes ideas from CTM and DMD, and incorporates human feedback to train a low-step genning model and LoRA.</li> <li><code>dmd2</code>: DMD says rather than rewarding the student model for following the teacher model step by step, only reward the final result and ignore how the student does it. This did great but came with problems like high hardware reqs and the student following the teacher step by step anyway, that DMD2 fixes:<ul> <li>Use the Two-Time Scale Rule inspired by this paper, reducing hardware reqs as DMD2 no longer requires generating an entire dataset using the teacher to stabilize training.</li> <li>Additionally use a discriminator as well, which potentially allows the student to surpass the teacher.</li> <li>1 step generation still isn't great, so they extend it to support many-step generation like how <code>lcm</code> does it (predict, noise inject, repeat).</li> </ul> </li> <li><code>pcm</code>: Again inspired by consistency models, rather than predicting the completely denoised image, cut it into multiple intermediate steps and try predicting from one to the next. This also solves the train-test mismatch of what the model was trained for not matching how it's used in practice.</li> </ul>"},{"location":"Sampling/07_tldr_usage/","title":"Practical TL;DR of Samplers","text":""},{"location":"Sampling/07_tldr_usage/#iteration-speed-how-long-1-step-takes","title":"Iteration Speed (How Long 1 Step Takes)","text":"Relative Speed Samplers 1x <code>euler</code>, <code>dpmpp_2m</code> (and everything with <code>&lt;number&gt;m</code> in the name), most others 2x <code>heun</code>, <code>dpm_2</code>, <code>dpmpp_sde</code>, <code>seeds_2</code>, <code>dpmpp_2s_ancestral</code>, <code>sa_solver_pece</code> (and everything with <code>2s</code> in the name) 3x <code>heunpp</code>, <code>seeds_3</code>, <code>dpm_adaptive</code>, anything with <code>3s</code> in the name <p>For example, this means that in the same time that you run <code>euler</code> for 20 steps, it takes about the same time to run <code>dpm_2</code> for 10 steps.</p> <p><code>dpm_fast</code> is a bit special; This sampler switches between the <code>DPM-3</code>, <code>DPM-2</code>, and <code>DPM-1</code> algorithm in a way that the total amount of time spent is the same as <code>euler</code> if you set the same amount of steps. Each step may take differing amounts of time.</p>"},{"location":"Sampling/07_tldr_usage/#samplers-that-change-composition-as-you-increase-step-count","title":"Samplers That Change Composition As You Increase Step Count","text":"<ul> <li><code>ddpm</code> </li> <li><code>restart</code> </li> <li><code>seeds</code> </li> <li><code>sa_solver(_pece)</code></li> <li>Any sampler with <code>ancestral (a)</code> in the name</li> <li>Any sampler with <code>sde</code> in the name</li> </ul> <p>Generally, these samplers beat their non-composition-changing counterparts if you use higher step counts.</p>"},{"location":"Sampling/07_tldr_usage/#sdxl-what-works-at-various-step-counts","title":"SDXL - What Works at Various Step Counts","text":"Steps Works Well Usually Breaks 1-12 Training-based methods like <code>lcm</code>, <code>lightning</code>, <code>hyper</code> Everything else 12-20 Many (realistically, 6-10 steps will also leave very visible artifacts, but may occasionally be ok.) <code>ipndm(_v)</code>, <code>dpmpp_3m_sde</code>, <code>seeds_3</code> 20-30 Mostly everything <code>dpmpp_3m_sde</code> may still leave artifacts 30+ High-order methods like <code>ipndm(_v)</code> / Stochastic methods like <code>dpmpp_3m_sde</code> - <p>But <code>X</code> Worked/Broke For Me!</p> <p>These are very rough estimates. You should see these only as general rules of thumb on what works in low / high step environments. For example, many Illustrious derivative models contain DMD2, but they don't tell you. If done well, you'll hardly notice any artifacts, while being able to gen in the very low step regime no problem.</p>"},{"location":"Schedule/00_schedule/","title":"What is a Noise Schedule?","text":"<p>UNDER CONSTRUCTION</p> <p>This section is incomplete and subject to massive changes. </p> <p>What's The Difference Between Samplers and Schedulers?</p> <p>The sampler controls the way to remove that amount of noise. The scheduler controls how much noise to remove at each step.</p> <p>A noise schedule is a list of numbers that quantify the amount of noise that should be left in an image at a specific timestep. These numbers are usually denoted using sigma \\(\\sigma_t.\\) A scheduler generates a noise schedule according to the model, step count, among other parameters; Maybe through complex calculations, or maybe it's just a predefined list.  For generating images, \\(\\sigma_t\\) starts at a high value and ends at a low value, signifying that as you go, the image should become less and less noisy until you get a clean image.</p> <p>The highest and lowest values of \\(\\sigma_t\\) depends on the model's training schedule. This usually isn't something you need to worry about, as your frontend UI should handle that for you. Do note that some schedules were designed for specific models, and thus will produce suboptimal outputs when used on a different one.</p> <p>For practical training reasons, \\(\\sigma_\\text{min}\\) actually may not be 0 (which would represent a clean, no-noise image). A 0 is often added to the end of the list to represent that there should be no noise in the final result.</p>"},{"location":"Schedule/01_schedule_list/","title":"(Non-exhaustive) List of Schedulers","text":"<ul> <li><code>normal</code>: Time uniform sampling that includes both <code>\u03c3_max</code> and <code>\u03c3_min</code><ul> <li>In practice, this may lead to having a final tiny step (<code>\u03c3_min</code> to 0) that doesn't behave well</li> </ul> </li> <li><code>karras</code>: The schedule introduced in EDM by Karras et al.</li> <li><code>exponential</code>: A schedule where the <code>\u03c3</code> follows an exponential curve</li> <li><code>sgm_uniform</code>: <code>normal</code>, except that <code>\u03c3_min</code> is not included<ul> <li>You can probably replace <code>normal</code> with this one in all situations</li> <li>AFAICT, the naming comes from StabilityAI's generative-models repository.</li> </ul> </li> <li><code>simple</code>: comfy wrote the simplest scheduler they could think of for fun. In practice, basically the same as <code>sgm_uniform</code>.</li> <li><code>ddim_uniform</code>: The schedule used in the original DDIM paper</li> <li><code>beta</code>: Paper authors found the model settles general composition in the early steps while ironing out small details in the later steps, with \"nothing\" really happening in the middle steps. They thus argue to use the beta distribution for generating a schedule, which would give more steps to both ends and leave a few steps in the middle.</li> <li><code>kl_optimal</code>: Introduced in AYS paper that theoretically would minimize the kl divergence.</li> <li><code>AYS</code> (Align Your Steps): Predefined lists of noise levels found through numerical optimization to be good for a specific model architecture.<ul> <li><code>SD1</code> / <code>SDXL</code> / <code>SVD</code>: Match this with the one your model is based on. </li> <li><code>AYS 11</code> / <code>AYS 32</code>: <code>AYS</code> tuned for 10 / 31 steps generation. If you select any step count besides 10 / 31, log-linear interpolation is used to create the undefined noise levels.</li> </ul> </li> <li><code>GITS</code> (Geometry-Inspired Time Scheduling): Paper authors examine different imagegen neural nets and found surprisingly similar generation trajectories across them. They exploit the structure of this trajectory, which is GITS.</li> </ul>"}]}